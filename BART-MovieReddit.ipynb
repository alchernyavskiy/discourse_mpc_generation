{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../groundhog/data/threads_with_metas_3ut_aug_full.pkl', 'rb') as f:\n",
    "    threads = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../groundhog/data/threads_with_metas_3ut_aug_new.pkl', 'rb') as f:\n",
    "    threads_new, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39803, 22608)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threads), len(threads_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads += threads_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_set = set()\n",
    "speakers_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discourse_tokens(discourse_list):\n",
    "    return [f'<u{discourse_list[1]+1}>', f'<to:u{discourse_list[0]+1}>', f'<{discourse_list[2]}>']\n",
    "\n",
    "\n",
    "def get_aug_value(ut, speaker='<s1>'):\n",
    "    return ' '.join([speaker] + get_discourse_tokens(ut['discourse']) + [ut['text']])\n",
    "\n",
    "\n",
    "def get_aug_value_nodis(ut, speaker='<s1>'):\n",
    "    return ' '.join([speaker] + get_discourse_tokens(ut['discourse'])[:-1] + [ut['text']])\n",
    "\n",
    "def preproc_text(text, utt_set, speakers_set):\n",
    "    utt_set |= set(re.findall(r'<u\\d+>', text))\n",
    "    speakers_set |= set(re.findall(r'<s\\d+>', text))\n",
    "    if type(text) == str:\n",
    "        res = re.sub(r'\\s+', ' ', str(text))\n",
    "        if len(res.strip()) == 0:\n",
    "            return 'unk'\n",
    "        return res.strip()\n",
    "    return 'unk'\n",
    "\n",
    "\n",
    "def get_dialogue_instances(threads, utt_set, speakers_set):\n",
    "    utter_covered = set() # кажду реплику генерим только один раз\n",
    "    \n",
    "    result = []\n",
    "    for thr in tqdm(threads):\n",
    "        try:\n",
    "            speakers = {}\n",
    "\n",
    "            for i, ut in enumerate(thr['dialogue']):\n",
    "                speaker = ut['speaker']\n",
    "                if speaker not in speakers:\n",
    "                    speakers[speaker] = '<s' + str(len(speakers) + 1) + '>'\n",
    "\n",
    "                if i >= 2:\n",
    "                    if thr['id'] + '_' + ut['id'] not in utter_covered:\n",
    "                        utter_covered.add(thr['id'] + '_' + ut['id'])\n",
    "\n",
    "                        utter_dict = {\n",
    "                            'thread_id': thr['id'],\n",
    "                            'id': thr['id'] + '_' + ut['id'],\n",
    "                            'history': f' {sep_token} '.join([get_aug_value_nodis(ut_his, speakers[ut_his['speaker']]) for\n",
    "                                                             ut_his in thr['dialogue'][:i]] + [speakers[ut['speaker']]]),\n",
    "                            'history_aug': f' {sep_token} '.join([get_aug_value(ut_his, speakers[ut_his['speaker']]) for\n",
    "                                                                  ut_his in thr['dialogue'][:i]] +\n",
    "                                                                 [' '.join(get_aug_value(ut, speakers[ut['speaker']]).split()[:3])]),\n",
    "                            'response': ut['text'],\n",
    "                            'response_aug': ' '.join(get_aug_value(ut, speakers[ut['speaker']]).split()[3:]),\n",
    "                        }\n",
    "\n",
    "                        for k in utter_dict:\n",
    "                            try:\n",
    "                                utter_dict[k] = preproc_text(utter_dict[k], utt_set, speakers_set)\n",
    "                            except:\n",
    "                                utter_dict[k] = 'unk'\n",
    "\n",
    "                        if len(utter_dict['response']) > 3:\n",
    "                            result.append(utter_dict)\n",
    "        except:\n",
    "            continue\n",
    "                    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62411/62411 [00:22<00:00, 2715.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dialogue_df = get_dialogue_instances(threads, utt_set, speakers_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111202, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by thread ids\n",
    "train_threads, val_threads = train_test_split(list(dialogue_df['thread_id'].unique()), test_size=0.1, random_state=575)\n",
    "train_df = dialogue_df[dialogue_df.thread_id.isin(train_threads)]\n",
    "val_df = dialogue_df[dialogue_df.thread_id.isin(val_threads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100621, 6), (10581, 6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thread_id</th>\n",
       "      <th>id</th>\n",
       "      <th>history</th>\n",
       "      <th>history_aug</th>\n",
       "      <th>response</th>\n",
       "      <th>response_aug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_zm90dt</td>\n",
       "      <td>t3_zm90dt_j0agl5x</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; I don’t think Henry is going...</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; &lt;init&gt; I don’t think Henry i...</td>\n",
       "      <td>Yup. They burnt that bridge into ashes at this...</td>\n",
       "      <td>&lt;answer&gt; Yup. They burnt that bridge into ashe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_zm90dt</td>\n",
       "      <td>t3_zm90dt_j0bhhx4</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; I don’t think Henry is going...</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; &lt;init&gt; I don’t think Henry i...</td>\n",
       "      <td>What's james gonna say, that he completely fuc...</td>\n",
       "      <td>&lt;question&gt; What's james gonna say, that he com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_zm90dt</td>\n",
       "      <td>t3_zm90dt_j0c0y3j</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; I don’t think Henry is going...</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; &lt;init&gt; I don’t think Henry i...</td>\n",
       "      <td>The Rock did that. It was up to Gunn to make t...</td>\n",
       "      <td>&lt;answer&gt; The Rock did that. It was up to Gunn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_zm90dt</td>\n",
       "      <td>t3_zm90dt_j0zisqb</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; I don’t think Henry is going...</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; &lt;init&gt; I don’t think Henry i...</td>\n",
       "      <td>So many people putting so much power into The ...</td>\n",
       "      <td>&lt;elaboration&gt; So many people putting so much p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_zm90dt</td>\n",
       "      <td>t3_zm90dt_j0zj4qg</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; I don’t think Henry is going...</td>\n",
       "      <td>&lt;s1&gt; &lt;u1&gt; &lt;to:u1&gt; &lt;init&gt; I don’t think Henry i...</td>\n",
       "      <td>And he didn’t even turn in a movie that made t...</td>\n",
       "      <td>&lt;elaboration&gt; And he didn’t even turn in a mov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   thread_id                 id  \\\n",
       "0  t3_zm90dt  t3_zm90dt_j0agl5x   \n",
       "1  t3_zm90dt  t3_zm90dt_j0bhhx4   \n",
       "2  t3_zm90dt  t3_zm90dt_j0c0y3j   \n",
       "3  t3_zm90dt  t3_zm90dt_j0zisqb   \n",
       "4  t3_zm90dt  t3_zm90dt_j0zj4qg   \n",
       "\n",
       "                                             history  \\\n",
       "0  <s1> <u1> <to:u1> I don’t think Henry is going...   \n",
       "1  <s1> <u1> <to:u1> I don’t think Henry is going...   \n",
       "2  <s1> <u1> <to:u1> I don’t think Henry is going...   \n",
       "3  <s1> <u1> <to:u1> I don’t think Henry is going...   \n",
       "4  <s1> <u1> <to:u1> I don’t think Henry is going...   \n",
       "\n",
       "                                         history_aug  \\\n",
       "0  <s1> <u1> <to:u1> <init> I don’t think Henry i...   \n",
       "1  <s1> <u1> <to:u1> <init> I don’t think Henry i...   \n",
       "2  <s1> <u1> <to:u1> <init> I don’t think Henry i...   \n",
       "3  <s1> <u1> <to:u1> <init> I don’t think Henry i...   \n",
       "4  <s1> <u1> <to:u1> <init> I don’t think Henry i...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Yup. They burnt that bridge into ashes at this...   \n",
       "1  What's james gonna say, that he completely fuc...   \n",
       "2  The Rock did that. It was up to Gunn to make t...   \n",
       "3  So many people putting so much power into The ...   \n",
       "4  And he didn’t even turn in a movie that made t...   \n",
       "\n",
       "                                        response_aug  \n",
       "0  <answer> Yup. They burnt that bridge into ashe...  \n",
       "1  <question> What's james gonna say, that he com...  \n",
       "2  <answer> The Rock did that. It was up to Gunn ...  \n",
       "3  <elaboration> So many people putting so much p...  \n",
       "4  <elaboration> And he didn’t even turn in a mov...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s1> <u1> <to:u1> <init> I don’t think Henry is going to play a different DC character. His instagram post reads like he’s done with DC films, not that he’s going to be in something else just not as Superman. </s> <s2> <u2> <to:u1> <question> At this point hasn't WB burnt the bridge? Toying with the character for years he finally gets welcomed back and now he gets kicked out again?? </s> <s1> <u3> <to:u2>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['history_aug'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train_structure_reddit.csv', sep='\\t', index=False)\n",
    "val_df.to_csv('data/val_structure_reddit.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_special_tokens = ['<negativereaction>',\n",
    "     '<other>',\n",
    "     '<appreciation>',\n",
    "     '<unk>',\n",
    "     '<elaboration>',\n",
    "     '<answer>',\n",
    "     '<question>',\n",
    "     '<humor>',\n",
    "     '<announcement>',\n",
    "     '<agreement>',\n",
    "     '<disagreement>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_s, max_u = (40, 41)\n",
    "for s in range(1, max_s+1):\n",
    "    additional_special_tokens.append('<s' + str(s) + '>')\n",
    "for u in range(1, max_u+1):\n",
    "    additional_special_tokens.append('<u' + str(u) + '>')\n",
    "for u in range(1, max_u+1):\n",
    "    additional_special_tokens.append('<to:u' + str(u) + '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': additional_special_tokens,\n",
    "                         'bos_token': '<s>',\n",
    "                         'eos_token': '</s>',\n",
    "                         'unk_token': '<unk>',\n",
    "                         'sep_token': '</s>',\n",
    "                         'pad_token': '<pad>',\n",
    "                         'cls_token': '<s>',\n",
    "                         'mask_token': '<mask>'}\n",
    "\n",
    "with open('data/special_tokens_map_reddit.pkl', 'wb') as f:\n",
    "    pickle.dump(special_tokens_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "model =  BartForConditionalGeneration.from_pretrained(model_name_or_path).to(device) # to check load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/special_tokens_map_reddit.pkl', 'rb') as f:\n",
    "    special_tokens_dict = pickle.load(f)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50397"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_toks + tokenizer.vocab_size # COPY TO WEIGHTS IN MODELING.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<negativereaction> <other> <appreciation> <elaboration> <answer> <question> <humor> <announcement> <agreement> <disagreement>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273, 50274])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/17/2023 11:18:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/17/2023 11:18:50 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/runs/May17_11-18-50_cn-014,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp,\n",
      "save_on_each_node=False,\n",
      "save_steps=80000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/17/2023 11:20:10 - WARNING - datasets.builder - Using custom data configuration default-1543fcdae0d91a45\n",
      "05/17/2023 11:20:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "05/17/2023 11:20:10 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "05/17/2023 11:20:10 - WARNING - datasets.builder - Reusing dataset csv (/home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "05/17/2023 11:20:10 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 163.11it/s]\n",
      "[INFO|configuration_utils.py:644] 2023-05-17 11:20:11,346 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-05-17 11:20:11,350 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,488 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,488 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /home/aschernyavskiy/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,489 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,489 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,489 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-17 11:20:13,489 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|configuration_utils.py:644] 2023-05-17 11:20:13,774 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-05-17 11:20:13,783 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2023-05-17 11:20:14,458 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/aschernyavskiy/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "CUSTOM BART with class_weight=0.0\n",
      "[INFO|modeling_utils.py:1694] 2023-05-17 11:20:22,223 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2023-05-17 11:20:22,223 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,264 >> Assigning ['<negativereaction>', '<other>', '<appreciation>', '<unk>', '<elaboration>', '<answer>', '<question>', '<humor>', '<announcement>', '<agreement>', '<disagreement>', '<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>', '<s17>', '<s18>', '<s19>', '<s20>', '<s21>', '<s22>', '<s23>', '<s24>', '<s25>', '<s26>', '<s27>', '<s28>', '<s29>', '<s30>', '<s31>', '<s32>', '<s33>', '<s34>', '<s35>', '<s36>', '<s37>', '<s38>', '<s39>', '<s40>', '<u1>', '<u2>', '<u3>', '<u4>', '<u5>', '<u6>', '<u7>', '<u8>', '<u9>', '<u10>', '<u11>', '<u12>', '<u13>', '<u14>', '<u15>', '<u16>', '<u17>', '<u18>', '<u19>', '<u20>', '<u21>', '<u22>', '<u23>', '<u24>', '<u25>', '<u26>', '<u27>', '<u28>', '<u29>', '<u30>', '<u31>', '<u32>', '<u33>', '<u34>', '<u35>', '<u36>', '<u37>', '<u38>', '<u39>', '<u40>', '<u41>', '<to:u1>', '<to:u2>', '<to:u3>', '<to:u4>', '<to:u5>', '<to:u6>', '<to:u7>', '<to:u8>', '<to:u9>', '<to:u10>', '<to:u11>', '<to:u12>', '<to:u13>', '<to:u14>', '<to:u15>', '<to:u16>', '<to:u17>', '<to:u18>', '<to:u19>', '<to:u20>', '<to:u21>', '<to:u22>', '<to:u23>', '<to:u24>', '<to:u25>', '<to:u26>', '<to:u27>', '<to:u28>', '<to:u29>', '<to:u30>', '<to:u31>', '<to:u32>', '<to:u33>', '<to:u34>', '<to:u35>', '<to:u36>', '<to:u37>', '<to:u38>', '<to:u39>', '<to:u40>', '<to:u41>'] to the additional_special_tokens key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,264 >> Adding <negativereaction> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,264 >> Adding <other> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,264 >> Adding <appreciation> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <elaboration> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <answer> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <question> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <humor> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <announcement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <agreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <disagreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,265 >> Adding <s10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,266 >> Adding <s30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <s40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,267 >> Adding <u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,268 >> Adding <u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <u41> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <to:u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <to:u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <to:u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,269 >> Adding <to:u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,270 >> Adding <to:u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-17 11:20:22,271 >> Adding <to:u41> to the vocabulary\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,272 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,292 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,293 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,293 >> Assigning </s> to the sep_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,293 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,294 >> Assigning <s> to the cls_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-17 11:20:22,294 >> Assigning <mask> to the mask_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/17/2023 11:20:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7a30ee22fc6c6edf.arrow\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/17/2023 11:20:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-af6bc30ff5c33de1.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:21<00:00,  1.93s/ba]\n",
      "/home/aschernyavskiy/anaconda3/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2023-05-17 11:20:56,931 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2023-05-17 11:20:56,932 >>   Num examples = 100621\n",
      "[INFO|trainer.py:1246] 2023-05-17 11:20:56,932 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1247] 2023-05-17 11:20:56,932 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1248] 2023-05-17 11:20:56,932 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1249] 2023-05-17 11:20:56,932 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1250] 2023-05-17 11:20:56,932 >>   Total optimization steps = 251550\n",
      "{'loss': 4.2309, 'learning_rate': 1.996024647187438e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.9582, 'learning_rate': 1.992049294374876e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.9859, 'learning_rate': 1.9880739415623137e-05, 'epoch': 0.03}        \n",
      "{'loss': 3.8924, 'learning_rate': 1.9840985887497518e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.8606, 'learning_rate': 1.9801232359371895e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.8811, 'learning_rate': 1.9761478831246276e-05, 'epoch': 0.06}        \n",
      "{'loss': 3.8797, 'learning_rate': 1.9721725303120653e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.8492, 'learning_rate': 1.968197177499503e-05, 'epoch': 0.08}         \n",
      "{'loss': 3.801, 'learning_rate': 1.964221824686941e-05, 'epoch': 0.09}          \n",
      "{'loss': 3.8813, 'learning_rate': 1.960246471874379e-05, 'epoch': 0.1}          \n",
      "{'loss': 3.8461, 'learning_rate': 1.956271119061817e-05, 'epoch': 0.11}         \n",
      "{'loss': 3.7967, 'learning_rate': 1.9522957662492547e-05, 'epoch': 0.12}        \n",
      "{'loss': 3.8492, 'learning_rate': 1.9483204134366928e-05, 'epoch': 0.13}        \n",
      "{'loss': 3.8902, 'learning_rate': 1.9443450606241305e-05, 'epoch': 0.14}        \n",
      "{'loss': 3.7633, 'learning_rate': 1.9403697078115683e-05, 'epoch': 0.15}        \n",
      "{'loss': 3.8006, 'learning_rate': 1.9363943549990063e-05, 'epoch': 0.16}        \n",
      "{'loss': 3.7636, 'learning_rate': 1.932419002186444e-05, 'epoch': 0.17}         \n",
      "{'loss': 3.7704, 'learning_rate': 1.928443649373882e-05, 'epoch': 0.18}         \n",
      "{'loss': 3.7562, 'learning_rate': 1.92446829656132e-05, 'epoch': 0.19}          \n",
      "{'loss': 3.7509, 'learning_rate': 1.920492943748758e-05, 'epoch': 0.2}          \n",
      "{'loss': 3.7898, 'learning_rate': 1.9165175909361957e-05, 'epoch': 0.21}        \n",
      "{'loss': 3.7811, 'learning_rate': 1.9125422381236335e-05, 'epoch': 0.22}        \n",
      "{'loss': 3.764, 'learning_rate': 1.9085668853110715e-05, 'epoch': 0.23}         \n",
      "{'loss': 3.7624, 'learning_rate': 1.9045915324985093e-05, 'epoch': 0.24}        \n",
      "{'loss': 3.7871, 'learning_rate': 1.9006161796859473e-05, 'epoch': 0.25}        \n",
      "{'loss': 3.7211, 'learning_rate': 1.896640826873385e-05, 'epoch': 0.26}         \n",
      "{'loss': 3.7812, 'learning_rate': 1.892665474060823e-05, 'epoch': 0.27}         \n",
      "{'loss': 3.802, 'learning_rate': 1.888690121248261e-05, 'epoch': 0.28}          \n",
      "{'loss': 3.7842, 'learning_rate': 1.8847147684356986e-05, 'epoch': 0.29}        \n",
      "{'loss': 3.7756, 'learning_rate': 1.8807394156231367e-05, 'epoch': 0.3}         \n",
      "{'loss': 3.7677, 'learning_rate': 1.8767640628105745e-05, 'epoch': 0.31}        \n",
      "{'loss': 3.721, 'learning_rate': 1.8727887099980125e-05, 'epoch': 0.32}         \n",
      "{'loss': 3.6707, 'learning_rate': 1.8688133571854503e-05, 'epoch': 0.33}        \n",
      "{'loss': 3.7914, 'learning_rate': 1.8648380043728883e-05, 'epoch': 0.34}        \n",
      "{'loss': 3.7504, 'learning_rate': 1.860862651560326e-05, 'epoch': 0.35}         \n",
      "{'loss': 3.7118, 'learning_rate': 1.8568872987477638e-05, 'epoch': 0.36}        \n",
      "{'loss': 3.7086, 'learning_rate': 1.852911945935202e-05, 'epoch': 0.37}         \n",
      "{'loss': 3.6724, 'learning_rate': 1.8489365931226396e-05, 'epoch': 0.38}        \n",
      "{'loss': 3.7158, 'learning_rate': 1.8449612403100777e-05, 'epoch': 0.39}        \n",
      "{'loss': 3.7693, 'learning_rate': 1.8409858874975155e-05, 'epoch': 0.4}         \n",
      "{'loss': 3.7613, 'learning_rate': 1.8370105346849535e-05, 'epoch': 0.41}        \n",
      "{'loss': 3.7426, 'learning_rate': 1.8330351818723913e-05, 'epoch': 0.42}        \n",
      "{'loss': 3.7426, 'learning_rate': 1.829059829059829e-05, 'epoch': 0.43}         \n",
      "{'loss': 3.7216, 'learning_rate': 1.825084476247267e-05, 'epoch': 0.44}         \n",
      "{'loss': 3.676, 'learning_rate': 1.8211091234347048e-05, 'epoch': 0.45}         \n",
      "{'loss': 3.7406, 'learning_rate': 1.817133770622143e-05, 'epoch': 0.46}         \n",
      "{'loss': 3.6801, 'learning_rate': 1.8131584178095806e-05, 'epoch': 0.47}        \n",
      "{'loss': 3.717, 'learning_rate': 1.8091830649970187e-05, 'epoch': 0.48}         \n",
      "{'loss': 3.7441, 'learning_rate': 1.8052077121844565e-05, 'epoch': 0.49}        \n",
      "{'loss': 3.7249, 'learning_rate': 1.8012323593718942e-05, 'epoch': 0.5}         \n",
      "{'loss': 3.6754, 'learning_rate': 1.7972570065593323e-05, 'epoch': 0.51}        \n",
      "{'loss': 3.7037, 'learning_rate': 1.79328165374677e-05, 'epoch': 0.52}          \n",
      "{'loss': 3.6811, 'learning_rate': 1.789306300934208e-05, 'epoch': 0.53}         \n",
      "{'loss': 3.6822, 'learning_rate': 1.7853309481216458e-05, 'epoch': 0.54}        \n",
      "{'loss': 3.6685, 'learning_rate': 1.781355595309084e-05, 'epoch': 0.55}         \n",
      "{'loss': 3.6721, 'learning_rate': 1.7773802424965216e-05, 'epoch': 0.56}        \n",
      "{'loss': 3.7507, 'learning_rate': 1.7734048896839594e-05, 'epoch': 0.57}        \n",
      "{'loss': 3.7174, 'learning_rate': 1.7694295368713975e-05, 'epoch': 0.58}        \n",
      "{'loss': 3.7456, 'learning_rate': 1.7654541840588352e-05, 'epoch': 0.59}        \n",
      "{'loss': 3.7036, 'learning_rate': 1.7614788312462733e-05, 'epoch': 0.6}         \n",
      "{'loss': 3.6957, 'learning_rate': 1.757503478433711e-05, 'epoch': 0.61}         \n",
      "{'loss': 3.6221, 'learning_rate': 1.753528125621149e-05, 'epoch': 0.62}         \n",
      "{'loss': 3.6422, 'learning_rate': 1.7495527728085868e-05, 'epoch': 0.63}        \n",
      "{'loss': 3.6765, 'learning_rate': 1.7455774199960246e-05, 'epoch': 0.64}        \n",
      "{'loss': 3.644, 'learning_rate': 1.7416020671834626e-05, 'epoch': 0.65}         \n",
      "{'loss': 3.701, 'learning_rate': 1.7376267143709004e-05, 'epoch': 0.66}         \n",
      "{'loss': 3.6559, 'learning_rate': 1.7336513615583385e-05, 'epoch': 0.67}        \n",
      "{'loss': 3.6529, 'learning_rate': 1.7296760087457762e-05, 'epoch': 0.68}        \n",
      "{'loss': 3.6155, 'learning_rate': 1.7257006559332143e-05, 'epoch': 0.69}        \n",
      "{'loss': 3.7022, 'learning_rate': 1.721725303120652e-05, 'epoch': 0.7}          \n",
      "{'loss': 3.6831, 'learning_rate': 1.7177499503080897e-05, 'epoch': 0.71}        \n",
      "{'loss': 3.6403, 'learning_rate': 1.7137745974955278e-05, 'epoch': 0.72}        \n",
      "{'loss': 3.6468, 'learning_rate': 1.7097992446829656e-05, 'epoch': 0.73}        \n",
      "{'loss': 3.6496, 'learning_rate': 1.7058238918704036e-05, 'epoch': 0.74}        \n",
      "{'loss': 3.6357, 'learning_rate': 1.7018485390578414e-05, 'epoch': 0.75}        \n",
      "{'loss': 3.6449, 'learning_rate': 1.6978731862452795e-05, 'epoch': 0.76}        \n",
      "{'loss': 3.69, 'learning_rate': 1.6938978334327172e-05, 'epoch': 0.77}          \n",
      "{'loss': 3.6338, 'learning_rate': 1.689922480620155e-05, 'epoch': 0.78}         \n",
      "{'loss': 3.6632, 'learning_rate': 1.685947127807593e-05, 'epoch': 0.79}         \n",
      "{'loss': 3.7027, 'learning_rate': 1.6819717749950307e-05, 'epoch': 0.8}         \n",
      "{'loss': 3.6733, 'learning_rate': 1.6779964221824688e-05, 'epoch': 0.81}        \n",
      "{'loss': 3.655, 'learning_rate': 1.674021069369907e-05, 'epoch': 0.81}          \n",
      "{'loss': 3.6429, 'learning_rate': 1.6700457165573446e-05, 'epoch': 0.82}        \n",
      "{'loss': 3.6412, 'learning_rate': 1.6660703637447827e-05, 'epoch': 0.83}        \n",
      "{'loss': 3.6386, 'learning_rate': 1.6620950109322205e-05, 'epoch': 0.84}        \n",
      "{'loss': 3.6712, 'learning_rate': 1.6581196581196585e-05, 'epoch': 0.85}        \n",
      "{'loss': 3.7027, 'learning_rate': 1.6541443053070963e-05, 'epoch': 0.86}        \n",
      "{'loss': 3.6405, 'learning_rate': 1.650168952494534e-05, 'epoch': 0.87}         \n",
      "{'loss': 3.6394, 'learning_rate': 1.646193599681972e-05, 'epoch': 0.88}         \n",
      "{'loss': 3.677, 'learning_rate': 1.6422182468694098e-05, 'epoch': 0.89}         \n",
      "{'loss': 3.722, 'learning_rate': 1.638242894056848e-05, 'epoch': 0.9}           \n",
      "{'loss': 3.6765, 'learning_rate': 1.6342675412442856e-05, 'epoch': 0.91}        \n",
      "{'loss': 3.6032, 'learning_rate': 1.6302921884317237e-05, 'epoch': 0.92}        \n",
      "{'loss': 3.6462, 'learning_rate': 1.6263168356191614e-05, 'epoch': 0.93}        \n",
      "{'loss': 3.6429, 'learning_rate': 1.6223414828065992e-05, 'epoch': 0.94}        \n",
      "{'loss': 3.6386, 'learning_rate': 1.6183661299940373e-05, 'epoch': 0.95}        \n",
      "{'loss': 3.7126, 'learning_rate': 1.614390777181475e-05, 'epoch': 0.96}         \n",
      "{'loss': 3.6476, 'learning_rate': 1.610415424368913e-05, 'epoch': 0.97}         \n",
      "{'loss': 3.599, 'learning_rate': 1.6064400715563508e-05, 'epoch': 0.98}         \n",
      "{'loss': 3.6481, 'learning_rate': 1.602464718743789e-05, 'epoch': 0.99}         \n",
      "{'loss': 3.5444, 'learning_rate': 1.5984893659312266e-05, 'epoch': 1.0}         \n",
      "{'loss': 3.4593, 'learning_rate': 1.5945140131186644e-05, 'epoch': 1.01}        \n",
      "{'loss': 3.3835, 'learning_rate': 1.5905386603061024e-05, 'epoch': 1.02}        \n",
      "{'loss': 3.4328, 'learning_rate': 1.5865633074935402e-05, 'epoch': 1.03}        \n",
      "{'loss': 3.4091, 'learning_rate': 1.5825879546809783e-05, 'epoch': 1.04}        \n",
      "{'loss': 3.4219, 'learning_rate': 1.578612601868416e-05, 'epoch': 1.05}         \n",
      "{'loss': 3.4443, 'learning_rate': 1.574637249055854e-05, 'epoch': 1.06}         \n",
      "{'loss': 3.4298, 'learning_rate': 1.5706618962432918e-05, 'epoch': 1.07}        \n",
      "{'loss': 3.4207, 'learning_rate': 1.5666865434307296e-05, 'epoch': 1.08}        \n",
      "{'loss': 3.4491, 'learning_rate': 1.5627111906181676e-05, 'epoch': 1.09}        \n",
      "{'loss': 3.4249, 'learning_rate': 1.5587358378056054e-05, 'epoch': 1.1}         \n",
      "{'loss': 3.4132, 'learning_rate': 1.5547604849930434e-05, 'epoch': 1.11}        \n",
      "{'loss': 3.4102, 'learning_rate': 1.5507851321804812e-05, 'epoch': 1.12}        \n",
      "{'loss': 3.43, 'learning_rate': 1.5468097793679193e-05, 'epoch': 1.13}          \n",
      "{'loss': 3.4627, 'learning_rate': 1.542834426555357e-05, 'epoch': 1.14}         \n",
      "{'loss': 3.4308, 'learning_rate': 1.5388590737427947e-05, 'epoch': 1.15}        \n",
      "{'loss': 3.4154, 'learning_rate': 1.5348837209302328e-05, 'epoch': 1.16}        \n",
      "{'loss': 3.467, 'learning_rate': 1.5309083681176706e-05, 'epoch': 1.17}         \n",
      "{'loss': 3.4189, 'learning_rate': 1.5269330153051086e-05, 'epoch': 1.18}        \n",
      "{'loss': 3.4684, 'learning_rate': 1.5229576624925464e-05, 'epoch': 1.19}        \n",
      "{'loss': 3.4449, 'learning_rate': 1.5189823096799843e-05, 'epoch': 1.2}         \n",
      "{'loss': 3.4934, 'learning_rate': 1.5150069568674222e-05, 'epoch': 1.21}        \n",
      "{'loss': 3.4387, 'learning_rate': 1.5110316040548601e-05, 'epoch': 1.22}        \n",
      "{'loss': 3.4408, 'learning_rate': 1.507056251242298e-05, 'epoch': 1.23}         \n",
      "{'loss': 3.4283, 'learning_rate': 1.5030808984297357e-05, 'epoch': 1.24}        \n",
      "{'loss': 3.4512, 'learning_rate': 1.4991055456171736e-05, 'epoch': 1.25}        \n",
      "{'loss': 3.455, 'learning_rate': 1.4951301928046116e-05, 'epoch': 1.26}         \n",
      "{'loss': 3.3813, 'learning_rate': 1.4911548399920495e-05, 'epoch': 1.27}        \n",
      "{'loss': 3.4033, 'learning_rate': 1.4871794871794874e-05, 'epoch': 1.28}        \n",
      "{'loss': 3.3979, 'learning_rate': 1.4832041343669253e-05, 'epoch': 1.29}        \n",
      "{'loss': 3.4727, 'learning_rate': 1.4792287815543632e-05, 'epoch': 1.3}         \n",
      "{'loss': 3.434, 'learning_rate': 1.475253428741801e-05, 'epoch': 1.31}          \n",
      "{'loss': 3.4699, 'learning_rate': 1.4712780759292388e-05, 'epoch': 1.32}        \n",
      "{'loss': 3.4286, 'learning_rate': 1.4673027231166767e-05, 'epoch': 1.33}        \n",
      "{'loss': 3.4099, 'learning_rate': 1.4633273703041146e-05, 'epoch': 1.34}        \n",
      "{'loss': 3.4459, 'learning_rate': 1.4593520174915526e-05, 'epoch': 1.35}        \n",
      "{'loss': 3.4841, 'learning_rate': 1.4553766646789905e-05, 'epoch': 1.36}        \n",
      "{'loss': 3.4846, 'learning_rate': 1.4514013118664284e-05, 'epoch': 1.37}        \n",
      "{'loss': 3.3981, 'learning_rate': 1.4474259590538661e-05, 'epoch': 1.38}        \n",
      "{'loss': 3.4652, 'learning_rate': 1.443450606241304e-05, 'epoch': 1.39}         \n",
      "{'loss': 3.4431, 'learning_rate': 1.439475253428742e-05, 'epoch': 1.4}          \n",
      "{'loss': 3.4455, 'learning_rate': 1.4354999006161798e-05, 'epoch': 1.41}        \n",
      "{'loss': 3.4893, 'learning_rate': 1.4315245478036177e-05, 'epoch': 1.42}        \n",
      "{'loss': 3.4916, 'learning_rate': 1.4275491949910556e-05, 'epoch': 1.43}        \n",
      "{'loss': 3.4399, 'learning_rate': 1.4235738421784936e-05, 'epoch': 1.44}        \n",
      "{'loss': 3.421, 'learning_rate': 1.4195984893659313e-05, 'epoch': 1.45}         \n",
      "{'loss': 3.4699, 'learning_rate': 1.4156231365533692e-05, 'epoch': 1.46}        \n",
      "{'loss': 3.4635, 'learning_rate': 1.4116477837408071e-05, 'epoch': 1.47}        \n",
      "{'loss': 3.4285, 'learning_rate': 1.407672430928245e-05, 'epoch': 1.48}         \n",
      "{'loss': 3.491, 'learning_rate': 1.403697078115683e-05, 'epoch': 1.49}          \n",
      "{'loss': 3.4598, 'learning_rate': 1.3997217253031208e-05, 'epoch': 1.5}         \n",
      "{'loss': 3.3739, 'learning_rate': 1.3957463724905587e-05, 'epoch': 1.51}        \n",
      "{'loss': 3.4472, 'learning_rate': 1.3917710196779965e-05, 'epoch': 1.52}        \n",
      "{'loss': 3.4658, 'learning_rate': 1.3877956668654344e-05, 'epoch': 1.53}        \n",
      "{'loss': 3.4581, 'learning_rate': 1.3838203140528723e-05, 'epoch': 1.54}        \n",
      "{'loss': 3.4953, 'learning_rate': 1.3798449612403102e-05, 'epoch': 1.55}        \n",
      "{'loss': 3.4912, 'learning_rate': 1.3758696084277481e-05, 'epoch': 1.56}        \n",
      "{'loss': 3.4516, 'learning_rate': 1.371894255615186e-05, 'epoch': 1.57}         \n",
      "{'loss': 3.4439, 'learning_rate': 1.367918902802624e-05, 'epoch': 1.58}         \n",
      "{'loss': 3.5023, 'learning_rate': 1.3639435499900617e-05, 'epoch': 1.59}        \n",
      " 32%|██████████▏                     | 80000/251550 [2:54:07<7:24:51,  6.43it/s][INFO|trainer.py:2090] 2023-05-17 14:15:04,647 >> Saving model checkpoint to checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000\n",
      "[INFO|configuration_utils.py:430] 2023-05-17 14:15:04,652 >> Configuration saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000/config.json\n",
      "[INFO|modeling_utils.py:1074] 2023-05-17 14:15:05,692 >> Model weights saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2023-05-17 14:15:05,859 >> tokenizer config file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2023-05-17 14:15:05,861 >> Special tokens file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2125] 2023-05-17 14:15:05,862 >> added tokens file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-80000/added_tokens.json\n",
      "{'loss': 3.4958, 'learning_rate': 1.3599681971774996e-05, 'epoch': 1.6}         \n",
      "{'loss': 3.4745, 'learning_rate': 1.3559928443649375e-05, 'epoch': 1.61}        \n",
      "{'loss': 3.4685, 'learning_rate': 1.3520174915523754e-05, 'epoch': 1.62}        \n",
      "{'loss': 3.4204, 'learning_rate': 1.3480421387398133e-05, 'epoch': 1.63}        \n",
      "{'loss': 3.4834, 'learning_rate': 1.3440667859272512e-05, 'epoch': 1.64}        \n",
      "{'loss': 3.4264, 'learning_rate': 1.3400914331146891e-05, 'epoch': 1.65}        \n",
      "{'loss': 3.4721, 'learning_rate': 1.3361160803021268e-05, 'epoch': 1.66}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4611, 'learning_rate': 1.3321407274895648e-05, 'epoch': 1.67}        \n",
      "{'loss': 3.4284, 'learning_rate': 1.3281653746770027e-05, 'epoch': 1.68}        \n",
      "{'loss': 3.4582, 'learning_rate': 1.3241900218644406e-05, 'epoch': 1.69}        \n",
      "{'loss': 3.4344, 'learning_rate': 1.3202146690518785e-05, 'epoch': 1.7}         \n",
      "{'loss': 3.4175, 'learning_rate': 1.3162393162393164e-05, 'epoch': 1.71}        \n",
      "{'loss': 3.3978, 'learning_rate': 1.3122639634267543e-05, 'epoch': 1.72}        \n",
      "{'loss': 3.4509, 'learning_rate': 1.308288610614192e-05, 'epoch': 1.73}         \n",
      "{'loss': 3.4422, 'learning_rate': 1.30431325780163e-05, 'epoch': 1.74}          \n",
      "{'loss': 3.4426, 'learning_rate': 1.3003379049890678e-05, 'epoch': 1.75}        \n",
      "{'loss': 3.4426, 'learning_rate': 1.2963625521765058e-05, 'epoch': 1.76}        \n",
      "{'loss': 3.4182, 'learning_rate': 1.2923871993639437e-05, 'epoch': 1.77}        \n",
      "{'loss': 3.4613, 'learning_rate': 1.2884118465513816e-05, 'epoch': 1.78}        \n",
      "{'loss': 3.4216, 'learning_rate': 1.2844364937388195e-05, 'epoch': 1.79}        \n",
      "{'loss': 3.4399, 'learning_rate': 1.2804611409262572e-05, 'epoch': 1.8}         \n",
      "{'loss': 3.4697, 'learning_rate': 1.2764857881136951e-05, 'epoch': 1.81}        \n",
      "{'loss': 3.4295, 'learning_rate': 1.272510435301133e-05, 'epoch': 1.82}         \n",
      "{'loss': 3.4833, 'learning_rate': 1.268535082488571e-05, 'epoch': 1.83}         \n",
      "{'loss': 3.4515, 'learning_rate': 1.2645597296760088e-05, 'epoch': 1.84}        \n",
      "{'loss': 3.4682, 'learning_rate': 1.2605843768634468e-05, 'epoch': 1.85}        \n",
      "{'loss': 3.4791, 'learning_rate': 1.2566090240508847e-05, 'epoch': 1.86}        \n",
      "{'loss': 3.4182, 'learning_rate': 1.2526336712383224e-05, 'epoch': 1.87}        \n",
      "{'loss': 3.4415, 'learning_rate': 1.2486583184257603e-05, 'epoch': 1.88}        \n",
      "{'loss': 3.5088, 'learning_rate': 1.2446829656131982e-05, 'epoch': 1.89}        \n",
      "{'loss': 3.3886, 'learning_rate': 1.2407076128006361e-05, 'epoch': 1.9}         \n",
      "{'loss': 3.5082, 'learning_rate': 1.236732259988074e-05, 'epoch': 1.91}         \n",
      "{'loss': 3.4111, 'learning_rate': 1.232756907175512e-05, 'epoch': 1.92}         \n",
      "{'loss': 3.4755, 'learning_rate': 1.2287815543629498e-05, 'epoch': 1.93}        \n",
      "{'loss': 3.3999, 'learning_rate': 1.2248062015503876e-05, 'epoch': 1.94}        \n",
      "{'loss': 3.4316, 'learning_rate': 1.2208308487378255e-05, 'epoch': 1.95}        \n",
      "{'loss': 3.4309, 'learning_rate': 1.2168554959252634e-05, 'epoch': 1.96}        \n",
      "{'loss': 3.4526, 'learning_rate': 1.2128801431127013e-05, 'epoch': 1.97}        \n",
      "{'loss': 3.4745, 'learning_rate': 1.2089047903001392e-05, 'epoch': 1.98}        \n",
      "{'loss': 3.4821, 'learning_rate': 1.2049294374875771e-05, 'epoch': 1.99}        \n",
      "{'loss': 3.4006, 'learning_rate': 1.200954084675015e-05, 'epoch': 2.0}          \n",
      "{'loss': 3.32, 'learning_rate': 1.1969787318624528e-05, 'epoch': 2.01}          \n",
      "{'loss': 3.2268, 'learning_rate': 1.1930033790498907e-05, 'epoch': 2.02}        \n",
      "{'loss': 3.2397, 'learning_rate': 1.1890280262373286e-05, 'epoch': 2.03}        \n",
      "{'loss': 3.2197, 'learning_rate': 1.1850526734247665e-05, 'epoch': 2.04}        \n",
      "{'loss': 3.1659, 'learning_rate': 1.1810773206122044e-05, 'epoch': 2.05}        \n",
      "{'loss': 3.2667, 'learning_rate': 1.1771019677996423e-05, 'epoch': 2.06}        \n",
      "{'loss': 3.2473, 'learning_rate': 1.1731266149870802e-05, 'epoch': 2.07}        \n",
      "{'loss': 3.28, 'learning_rate': 1.169151262174518e-05, 'epoch': 2.08}           \n",
      "{'loss': 3.2836, 'learning_rate': 1.1651759093619559e-05, 'epoch': 2.09}        \n",
      "{'loss': 3.2575, 'learning_rate': 1.1612005565493938e-05, 'epoch': 2.1}         \n",
      "{'loss': 3.233, 'learning_rate': 1.1572252037368317e-05, 'epoch': 2.11}         \n",
      "{'loss': 3.2267, 'learning_rate': 1.1532498509242696e-05, 'epoch': 2.12}        \n",
      "{'loss': 3.2537, 'learning_rate': 1.1492744981117075e-05, 'epoch': 2.13}        \n",
      "{'loss': 3.2476, 'learning_rate': 1.1452991452991454e-05, 'epoch': 2.14}        \n",
      "{'loss': 3.2917, 'learning_rate': 1.1413237924865831e-05, 'epoch': 2.15}        \n",
      "{'loss': 3.2559, 'learning_rate': 1.137348439674021e-05, 'epoch': 2.16}         \n",
      "{'loss': 3.2904, 'learning_rate': 1.133373086861459e-05, 'epoch': 2.17}         \n",
      "{'loss': 3.2339, 'learning_rate': 1.1293977340488969e-05, 'epoch': 2.18}        \n",
      "{'loss': 3.2273, 'learning_rate': 1.1254223812363348e-05, 'epoch': 2.19}        \n",
      "{'loss': 3.269, 'learning_rate': 1.1214470284237727e-05, 'epoch': 2.2}          \n",
      "{'loss': 3.2544, 'learning_rate': 1.1174716756112106e-05, 'epoch': 2.21}        \n",
      "{'loss': 3.2498, 'learning_rate': 1.1134963227986483e-05, 'epoch': 2.22}        \n",
      "{'loss': 3.2543, 'learning_rate': 1.1095209699860862e-05, 'epoch': 2.23}        \n",
      "{'loss': 3.2975, 'learning_rate': 1.1055456171735241e-05, 'epoch': 2.24}        \n",
      "{'loss': 3.201, 'learning_rate': 1.101570264360962e-05, 'epoch': 2.25}          \n",
      "{'loss': 3.2519, 'learning_rate': 1.0975949115484e-05, 'epoch': 2.26}           \n",
      "{'loss': 3.2498, 'learning_rate': 1.0936195587358379e-05, 'epoch': 2.27}        \n",
      "{'loss': 3.3126, 'learning_rate': 1.0896442059232758e-05, 'epoch': 2.28}        \n",
      "{'loss': 3.2587, 'learning_rate': 1.0856688531107135e-05, 'epoch': 2.29}        \n",
      "{'loss': 3.2762, 'learning_rate': 1.0816935002981514e-05, 'epoch': 2.3}         \n",
      "{'loss': 3.274, 'learning_rate': 1.0777181474855893e-05, 'epoch': 2.31}         \n",
      "{'loss': 3.2613, 'learning_rate': 1.0737427946730272e-05, 'epoch': 2.32}        \n",
      "{'loss': 3.237, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.33}         \n",
      "{'loss': 3.2866, 'learning_rate': 1.065792089047903e-05, 'epoch': 2.34}         \n",
      "{'loss': 3.289, 'learning_rate': 1.061816736235341e-05, 'epoch': 2.35}          \n",
      "{'loss': 3.2834, 'learning_rate': 1.0578413834227787e-05, 'epoch': 2.36}        \n",
      "{'loss': 3.2315, 'learning_rate': 1.0538660306102166e-05, 'epoch': 2.37}        \n",
      "{'loss': 3.2398, 'learning_rate': 1.0498906777976545e-05, 'epoch': 2.38}        \n",
      "{'loss': 3.2594, 'learning_rate': 1.0459153249850924e-05, 'epoch': 2.39}        \n",
      "{'loss': 3.2268, 'learning_rate': 1.0419399721725303e-05, 'epoch': 2.4}         \n",
      "{'loss': 3.2358, 'learning_rate': 1.0379646193599682e-05, 'epoch': 2.41}        \n",
      "{'loss': 3.2675, 'learning_rate': 1.0339892665474061e-05, 'epoch': 2.42}        \n",
      "{'loss': 3.3141, 'learning_rate': 1.0300139137348442e-05, 'epoch': 2.42}        \n",
      "{'loss': 3.2548, 'learning_rate': 1.0260385609222821e-05, 'epoch': 2.43}        \n",
      "{'loss': 3.2692, 'learning_rate': 1.0220632081097199e-05, 'epoch': 2.44}        \n",
      "{'loss': 3.3037, 'learning_rate': 1.0180878552971578e-05, 'epoch': 2.45}        \n",
      "{'loss': 3.2563, 'learning_rate': 1.0141125024845957e-05, 'epoch': 2.46}        \n",
      "{'loss': 3.3022, 'learning_rate': 1.0101371496720336e-05, 'epoch': 2.47}        \n",
      "{'loss': 3.285, 'learning_rate': 1.0061617968594715e-05, 'epoch': 2.48}         \n",
      "{'loss': 3.3, 'learning_rate': 1.0021864440469094e-05, 'epoch': 2.49}           \n",
      "{'loss': 3.2424, 'learning_rate': 9.982110912343471e-06, 'epoch': 2.5}          \n",
      "{'loss': 3.263, 'learning_rate': 9.94235738421785e-06, 'epoch': 2.51}           \n",
      "{'loss': 3.2913, 'learning_rate': 9.90260385609223e-06, 'epoch': 2.52}          \n",
      "{'loss': 3.2487, 'learning_rate': 9.862850327966607e-06, 'epoch': 2.53}         \n",
      "{'loss': 3.2543, 'learning_rate': 9.823096799840986e-06, 'epoch': 2.54}         \n",
      "{'loss': 3.2494, 'learning_rate': 9.783343271715365e-06, 'epoch': 2.55}         \n",
      "{'loss': 3.2771, 'learning_rate': 9.743589743589744e-06, 'epoch': 2.56}         \n",
      "{'loss': 3.3013, 'learning_rate': 9.703836215464123e-06, 'epoch': 2.57}         \n",
      "{'loss': 3.29, 'learning_rate': 9.664082687338502e-06, 'epoch': 2.58}           \n",
      "{'loss': 3.2884, 'learning_rate': 9.624329159212881e-06, 'epoch': 2.59}         \n",
      "{'loss': 3.2749, 'learning_rate': 9.584575631087259e-06, 'epoch': 2.6}          \n",
      "{'loss': 3.2846, 'learning_rate': 9.544822102961638e-06, 'epoch': 2.61}         \n",
      "{'loss': 3.3255, 'learning_rate': 9.505068574836017e-06, 'epoch': 2.62}         \n",
      "{'loss': 3.2273, 'learning_rate': 9.465315046710396e-06, 'epoch': 2.63}         \n",
      "{'loss': 3.2825, 'learning_rate': 9.425561518584775e-06, 'epoch': 2.64}         \n",
      "{'loss': 3.3129, 'learning_rate': 9.385807990459154e-06, 'epoch': 2.65}         \n",
      "{'loss': 3.3169, 'learning_rate': 9.346054462333533e-06, 'epoch': 2.66}         \n",
      "{'loss': 3.2828, 'learning_rate': 9.30630093420791e-06, 'epoch': 2.67}          \n",
      "{'loss': 3.2684, 'learning_rate': 9.26654740608229e-06, 'epoch': 2.68}          \n",
      "{'loss': 3.2639, 'learning_rate': 9.226793877956669e-06, 'epoch': 2.69}         \n",
      "{'loss': 3.2725, 'learning_rate': 9.187040349831048e-06, 'epoch': 2.7}          \n",
      "{'loss': 3.3525, 'learning_rate': 9.147286821705427e-06, 'epoch': 2.71}         \n",
      "{'loss': 3.2611, 'learning_rate': 9.107533293579806e-06, 'epoch': 2.72}         \n",
      "{'loss': 3.2728, 'learning_rate': 9.067779765454185e-06, 'epoch': 2.73}         \n",
      "{'loss': 3.2302, 'learning_rate': 9.028026237328562e-06, 'epoch': 2.74}         \n",
      "{'loss': 3.3107, 'learning_rate': 8.988272709202941e-06, 'epoch': 2.75}         \n",
      "{'loss': 3.2549, 'learning_rate': 8.94851918107732e-06, 'epoch': 2.76}          \n",
      "{'loss': 3.2961, 'learning_rate': 8.9087656529517e-06, 'epoch': 2.77}           \n",
      "{'loss': 3.2963, 'learning_rate': 8.869012124826079e-06, 'epoch': 2.78}         \n",
      " 56%|█████████████████▎             | 140222/251550 [5:35:42<5:13:28,  5.92it/s]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-base\" \\\n",
    "    --train_file=\"data/train_structure_reddit.csv\" \\\n",
    "    --validation_file=\"data/val_structure_reddit.csv\" \\\n",
    "    --text_column=\"history_aug\" \\\n",
    "    --summary_column=\"response_aug\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=0. \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/15/2023 11:55:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/15/2023 11:55:32 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/runs/May15_11-55-32_cn-014,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100,\n",
      "save_on_each_node=False,\n",
      "save_steps=80000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/15/2023 11:56:52 - WARNING - datasets.builder - Using custom data configuration default-1543fcdae0d91a45\n",
      "05/15/2023 11:56:52 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "05/15/2023 11:56:52 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "05/15/2023 11:56:52 - WARNING - datasets.builder - Reusing dataset csv (/home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "05/15/2023 11:56:52 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 177.73it/s]\n",
      "[INFO|configuration_utils.py:644] 2023-05-15 11:56:53,307 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-05-15 11:56:53,310 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,566 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,567 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /home/aschernyavskiy/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,567 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,567 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,567 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-05-15 11:56:55,567 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|configuration_utils.py:644] 2023-05-15 11:56:55,855 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-05-15 11:56:55,857 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2023-05-15 11:56:56,370 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/aschernyavskiy/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "CUSTOM BART with class_weight=100.0\n",
      "[INFO|modeling_utils.py:1694] 2023-05-15 11:57:02,066 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2023-05-15 11:57:02,066 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,069 >> Assigning ['<negativereaction>', '<other>', '<appreciation>', '<unk>', '<elaboration>', '<answer>', '<question>', '<humor>', '<announcement>', '<agreement>', '<disagreement>', '<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>', '<s17>', '<s18>', '<s19>', '<s20>', '<s21>', '<s22>', '<s23>', '<s24>', '<s25>', '<s26>', '<s27>', '<s28>', '<s29>', '<s30>', '<s31>', '<s32>', '<s33>', '<s34>', '<s35>', '<s36>', '<s37>', '<s38>', '<s39>', '<s40>', '<u1>', '<u2>', '<u3>', '<u4>', '<u5>', '<u6>', '<u7>', '<u8>', '<u9>', '<u10>', '<u11>', '<u12>', '<u13>', '<u14>', '<u15>', '<u16>', '<u17>', '<u18>', '<u19>', '<u20>', '<u21>', '<u22>', '<u23>', '<u24>', '<u25>', '<u26>', '<u27>', '<u28>', '<u29>', '<u30>', '<u31>', '<u32>', '<u33>', '<u34>', '<u35>', '<u36>', '<u37>', '<u38>', '<u39>', '<u40>', '<u41>', '<to:u1>', '<to:u2>', '<to:u3>', '<to:u4>', '<to:u5>', '<to:u6>', '<to:u7>', '<to:u8>', '<to:u9>', '<to:u10>', '<to:u11>', '<to:u12>', '<to:u13>', '<to:u14>', '<to:u15>', '<to:u16>', '<to:u17>', '<to:u18>', '<to:u19>', '<to:u20>', '<to:u21>', '<to:u22>', '<to:u23>', '<to:u24>', '<to:u25>', '<to:u26>', '<to:u27>', '<to:u28>', '<to:u29>', '<to:u30>', '<to:u31>', '<to:u32>', '<to:u33>', '<to:u34>', '<to:u35>', '<to:u36>', '<to:u37>', '<to:u38>', '<to:u39>', '<to:u40>', '<to:u41>'] to the additional_special_tokens key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,070 >> Adding <negativereaction> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <other> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <appreciation> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <elaboration> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <answer> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <question> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <humor> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <announcement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <agreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <disagreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,073 >> Adding <s6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,074 >> Adding <s24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <s40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,075 >> Adding <u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,076 >> Adding <u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,077 >> Adding <u38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <u39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <u40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <u41> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,078 >> Adding <to:u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,079 >> Adding <to:u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-05-15 11:57:02,080 >> Adding <to:u41> to the vocabulary\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,080 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,083 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,083 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,083 >> Assigning </s> to the sep_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,084 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,084 >> Assigning <s> to the cls_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-05-15 11:57:02,085 >> Assigning <mask> to the mask_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|               | 0/101 [00:00<?, ?ba/s]05/15/2023 11:57:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7a30ee22fc6c6edf.arrow\n",
      "Running tokenizer on train dataset: 100%|█████| 101/101 [03:05<00:00,  1.83s/ba]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/15/2023 12:00:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-1543fcdae0d91a45/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7933951e04031a88.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:19<00:00,  1.77s/ba]\n",
      "/home/aschernyavskiy/anaconda3/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2023-05-15 12:00:30,878 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2023-05-15 12:00:30,881 >>   Num examples = 100621\n",
      "[INFO|trainer.py:1246] 2023-05-15 12:00:30,881 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1247] 2023-05-15 12:00:30,881 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1248] 2023-05-15 12:00:30,881 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1249] 2023-05-15 12:00:30,881 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1250] 2023-05-15 12:00:30,881 >>   Total optimization steps = 251550\n",
      "{'loss': 3.1384, 'learning_rate': 1.996024647187438e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7406, 'learning_rate': 1.992049294374876e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6055, 'learning_rate': 1.9880739415623137e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.5767, 'learning_rate': 1.9840985887497518e-05, 'epoch': 0.04}        \n",
      "{'loss': 2.5131, 'learning_rate': 1.9801232359371895e-05, 'epoch': 0.05}        \n",
      "{'loss': 2.4252, 'learning_rate': 1.9761478831246276e-05, 'epoch': 0.06}        \n",
      "{'loss': 2.4636, 'learning_rate': 1.9721725303120653e-05, 'epoch': 0.07}        \n",
      "{'loss': 2.3931, 'learning_rate': 1.968197177499503e-05, 'epoch': 0.08}         \n",
      "{'loss': 2.4908, 'learning_rate': 1.964221824686941e-05, 'epoch': 0.09}         \n",
      "{'loss': 2.5087, 'learning_rate': 1.960246471874379e-05, 'epoch': 0.1}          \n",
      "{'loss': 2.3427, 'learning_rate': 1.956271119061817e-05, 'epoch': 0.11}         \n",
      "{'loss': 2.3396, 'learning_rate': 1.9522957662492547e-05, 'epoch': 0.12}        \n",
      "{'loss': 2.4183, 'learning_rate': 1.9483204134366928e-05, 'epoch': 0.13}        \n",
      "{'loss': 2.4422, 'learning_rate': 1.9443450606241305e-05, 'epoch': 0.14}        \n",
      "{'loss': 2.3228, 'learning_rate': 1.9403697078115683e-05, 'epoch': 0.15}        \n",
      "{'loss': 2.4069, 'learning_rate': 1.9363943549990063e-05, 'epoch': 0.16}        \n",
      "{'loss': 2.3315, 'learning_rate': 1.932419002186444e-05, 'epoch': 0.17}         \n",
      "{'loss': 2.4049, 'learning_rate': 1.928443649373882e-05, 'epoch': 0.18}         \n",
      "{'loss': 2.3037, 'learning_rate': 1.92446829656132e-05, 'epoch': 0.19}          \n",
      "{'loss': 2.3176, 'learning_rate': 1.920492943748758e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.3124, 'learning_rate': 1.9165175909361957e-05, 'epoch': 0.21}        \n",
      "{'loss': 2.3287, 'learning_rate': 1.9125422381236335e-05, 'epoch': 0.22}        \n",
      "{'loss': 2.3038, 'learning_rate': 1.9085668853110715e-05, 'epoch': 0.23}        \n",
      "{'loss': 2.3256, 'learning_rate': 1.9045915324985093e-05, 'epoch': 0.24}        \n",
      "{'loss': 2.431, 'learning_rate': 1.9006161796859473e-05, 'epoch': 0.25}         \n",
      "{'loss': 2.3341, 'learning_rate': 1.896640826873385e-05, 'epoch': 0.26}         \n",
      "{'loss': 2.2906, 'learning_rate': 1.892665474060823e-05, 'epoch': 0.27}         \n",
      "{'loss': 2.3062, 'learning_rate': 1.888690121248261e-05, 'epoch': 0.28}         \n",
      "{'loss': 2.2373, 'learning_rate': 1.8847147684356986e-05, 'epoch': 0.29}        \n",
      "{'loss': 2.3594, 'learning_rate': 1.8807394156231367e-05, 'epoch': 0.3}         \n",
      "{'loss': 2.4089, 'learning_rate': 1.8767640628105745e-05, 'epoch': 0.31}        \n",
      "{'loss': 2.334, 'learning_rate': 1.8727887099980125e-05, 'epoch': 0.32}         \n",
      "{'loss': 2.2829, 'learning_rate': 1.8688133571854503e-05, 'epoch': 0.33}        \n",
      "{'loss': 2.2711, 'learning_rate': 1.8648380043728883e-05, 'epoch': 0.34}        \n",
      "{'loss': 2.2683, 'learning_rate': 1.860862651560326e-05, 'epoch': 0.35}         \n",
      "{'loss': 2.3747, 'learning_rate': 1.8568872987477638e-05, 'epoch': 0.36}        \n",
      "{'loss': 2.2781, 'learning_rate': 1.852911945935202e-05, 'epoch': 0.37}         \n",
      "{'loss': 2.2977, 'learning_rate': 1.8489365931226396e-05, 'epoch': 0.38}        \n",
      "{'loss': 2.2904, 'learning_rate': 1.8449612403100777e-05, 'epoch': 0.39}        \n",
      "{'loss': 2.3023, 'learning_rate': 1.8409858874975155e-05, 'epoch': 0.4}         \n",
      "{'loss': 2.2434, 'learning_rate': 1.8370105346849535e-05, 'epoch': 0.41}        \n",
      "{'loss': 2.2614, 'learning_rate': 1.8330351818723913e-05, 'epoch': 0.42}        \n",
      "{'loss': 2.2842, 'learning_rate': 1.829059829059829e-05, 'epoch': 0.43}         \n",
      "{'loss': 2.2688, 'learning_rate': 1.825084476247267e-05, 'epoch': 0.44}         \n",
      "{'loss': 2.2801, 'learning_rate': 1.8211091234347048e-05, 'epoch': 0.45}        \n",
      "{'loss': 2.3041, 'learning_rate': 1.817133770622143e-05, 'epoch': 0.46}         \n",
      "{'loss': 2.2794, 'learning_rate': 1.8131584178095806e-05, 'epoch': 0.47}        \n",
      "{'loss': 2.3048, 'learning_rate': 1.8091830649970187e-05, 'epoch': 0.48}        \n",
      "{'loss': 2.2508, 'learning_rate': 1.8052077121844565e-05, 'epoch': 0.49}        \n",
      "{'loss': 2.2194, 'learning_rate': 1.8012323593718942e-05, 'epoch': 0.5}         \n",
      "{'loss': 2.2983, 'learning_rate': 1.7972570065593323e-05, 'epoch': 0.51}        \n",
      "{'loss': 2.2951, 'learning_rate': 1.79328165374677e-05, 'epoch': 0.52}          \n",
      "{'loss': 2.2789, 'learning_rate': 1.789306300934208e-05, 'epoch': 0.53}         \n",
      "{'loss': 2.2903, 'learning_rate': 1.7853309481216458e-05, 'epoch': 0.54}        \n",
      "{'loss': 2.2813, 'learning_rate': 1.781355595309084e-05, 'epoch': 0.55}         \n",
      "{'loss': 2.2858, 'learning_rate': 1.7773802424965216e-05, 'epoch': 0.56}        \n",
      "{'loss': 2.2878, 'learning_rate': 1.7734048896839594e-05, 'epoch': 0.57}        \n",
      "{'loss': 2.2757, 'learning_rate': 1.7694295368713975e-05, 'epoch': 0.58}        \n",
      "{'loss': 2.3176, 'learning_rate': 1.7654541840588352e-05, 'epoch': 0.59}        \n",
      "{'loss': 2.2558, 'learning_rate': 1.7614788312462733e-05, 'epoch': 0.6}         \n",
      "{'loss': 2.2677, 'learning_rate': 1.757503478433711e-05, 'epoch': 0.61}         \n",
      "{'loss': 2.2381, 'learning_rate': 1.753528125621149e-05, 'epoch': 0.62}         \n",
      "{'loss': 2.2215, 'learning_rate': 1.7495527728085868e-05, 'epoch': 0.63}        \n",
      "{'loss': 2.3091, 'learning_rate': 1.7455774199960246e-05, 'epoch': 0.64}        \n",
      "{'loss': 2.2296, 'learning_rate': 1.7416020671834626e-05, 'epoch': 0.65}        \n",
      "{'loss': 2.1603, 'learning_rate': 1.7376267143709004e-05, 'epoch': 0.66}        \n",
      "{'loss': 2.2448, 'learning_rate': 1.7336513615583385e-05, 'epoch': 0.67}        \n",
      "{'loss': 2.2509, 'learning_rate': 1.7296760087457762e-05, 'epoch': 0.68}        \n",
      "{'loss': 2.232, 'learning_rate': 1.7257006559332143e-05, 'epoch': 0.69}         \n",
      "{'loss': 2.2026, 'learning_rate': 1.721725303120652e-05, 'epoch': 0.7}          \n",
      "{'loss': 2.2485, 'learning_rate': 1.7177499503080897e-05, 'epoch': 0.71}        \n",
      "{'loss': 2.2121, 'learning_rate': 1.7137745974955278e-05, 'epoch': 0.72}        \n",
      "{'loss': 2.2442, 'learning_rate': 1.7097992446829656e-05, 'epoch': 0.73}        \n",
      "{'loss': 2.2656, 'learning_rate': 1.7058238918704036e-05, 'epoch': 0.74}        \n",
      "{'loss': 2.1492, 'learning_rate': 1.7018485390578414e-05, 'epoch': 0.75}        \n",
      "{'loss': 2.1972, 'learning_rate': 1.6978731862452795e-05, 'epoch': 0.76}        \n",
      "{'loss': 2.2831, 'learning_rate': 1.6938978334327172e-05, 'epoch': 0.77}        \n",
      "{'loss': 2.2896, 'learning_rate': 1.689922480620155e-05, 'epoch': 0.78}         \n",
      "{'loss': 2.2245, 'learning_rate': 1.685947127807593e-05, 'epoch': 0.79}         \n",
      "{'loss': 2.1627, 'learning_rate': 1.6819717749950307e-05, 'epoch': 0.8}         \n",
      "{'loss': 2.2142, 'learning_rate': 1.6779964221824688e-05, 'epoch': 0.81}        \n",
      "{'loss': 2.2106, 'learning_rate': 1.674021069369907e-05, 'epoch': 0.81}         \n",
      "{'loss': 2.191, 'learning_rate': 1.6700457165573446e-05, 'epoch': 0.82}         \n",
      "{'loss': 2.2559, 'learning_rate': 1.6660703637447827e-05, 'epoch': 0.83}        \n",
      "{'loss': 2.2219, 'learning_rate': 1.6620950109322205e-05, 'epoch': 0.84}        \n",
      "{'loss': 2.1778, 'learning_rate': 1.6581196581196585e-05, 'epoch': 0.85}        \n",
      "{'loss': 2.2517, 'learning_rate': 1.6541443053070963e-05, 'epoch': 0.86}        \n",
      "{'loss': 2.2552, 'learning_rate': 1.650168952494534e-05, 'epoch': 0.87}         \n",
      "{'loss': 2.2148, 'learning_rate': 1.646193599681972e-05, 'epoch': 0.88}         \n",
      "{'loss': 2.264, 'learning_rate': 1.6422182468694098e-05, 'epoch': 0.89}         \n",
      "{'loss': 2.2926, 'learning_rate': 1.638242894056848e-05, 'epoch': 0.9}          \n",
      "{'loss': 2.2385, 'learning_rate': 1.6342675412442856e-05, 'epoch': 0.91}        \n",
      "{'loss': 2.1448, 'learning_rate': 1.6302921884317237e-05, 'epoch': 0.92}        \n",
      "{'loss': 2.2712, 'learning_rate': 1.6263168356191614e-05, 'epoch': 0.93}        \n",
      "{'loss': 2.2136, 'learning_rate': 1.6223414828065992e-05, 'epoch': 0.94}        \n",
      "{'loss': 2.2063, 'learning_rate': 1.6183661299940373e-05, 'epoch': 0.95}        \n",
      "{'loss': 2.1979, 'learning_rate': 1.614390777181475e-05, 'epoch': 0.96}         \n",
      "{'loss': 2.2474, 'learning_rate': 1.610415424368913e-05, 'epoch': 0.97}         \n",
      "{'loss': 2.1543, 'learning_rate': 1.6064400715563508e-05, 'epoch': 0.98}        \n",
      "{'loss': 2.2133, 'learning_rate': 1.602464718743789e-05, 'epoch': 0.99}         \n",
      "{'loss': 2.1756, 'learning_rate': 1.5984893659312266e-05, 'epoch': 1.0}         \n",
      "{'loss': 2.1334, 'learning_rate': 1.5945140131186644e-05, 'epoch': 1.01}        \n",
      "{'loss': 2.1165, 'learning_rate': 1.5905386603061024e-05, 'epoch': 1.02}        \n",
      "{'loss': 2.0362, 'learning_rate': 1.5865633074935402e-05, 'epoch': 1.03}        \n",
      "{'loss': 2.1004, 'learning_rate': 1.5825879546809783e-05, 'epoch': 1.04}        \n",
      "{'loss': 2.1029, 'learning_rate': 1.578612601868416e-05, 'epoch': 1.05}         \n",
      "{'loss': 2.1649, 'learning_rate': 1.574637249055854e-05, 'epoch': 1.06}         \n",
      "{'loss': 2.1731, 'learning_rate': 1.5706618962432918e-05, 'epoch': 1.07}        \n",
      "{'loss': 2.0675, 'learning_rate': 1.5666865434307296e-05, 'epoch': 1.08}        \n",
      "{'loss': 2.1402, 'learning_rate': 1.5627111906181676e-05, 'epoch': 1.09}        \n",
      "{'loss': 2.204, 'learning_rate': 1.5587358378056054e-05, 'epoch': 1.1}          \n",
      "{'loss': 2.1157, 'learning_rate': 1.5547604849930434e-05, 'epoch': 1.11}        \n",
      "{'loss': 2.136, 'learning_rate': 1.5507851321804812e-05, 'epoch': 1.12}         \n",
      "{'loss': 2.1522, 'learning_rate': 1.5468097793679193e-05, 'epoch': 1.13}        \n",
      "{'loss': 2.2141, 'learning_rate': 1.542834426555357e-05, 'epoch': 1.14}         \n",
      "{'loss': 2.1028, 'learning_rate': 1.5388590737427947e-05, 'epoch': 1.15}        \n",
      "{'loss': 2.0672, 'learning_rate': 1.5348837209302328e-05, 'epoch': 1.16}        \n",
      "{'loss': 2.0906, 'learning_rate': 1.5309083681176706e-05, 'epoch': 1.17}        \n",
      "{'loss': 2.1768, 'learning_rate': 1.5269330153051086e-05, 'epoch': 1.18}        \n",
      "{'loss': 2.1231, 'learning_rate': 1.5229576624925464e-05, 'epoch': 1.19}        \n",
      "{'loss': 2.0897, 'learning_rate': 1.5189823096799843e-05, 'epoch': 1.2}         \n",
      "{'loss': 2.099, 'learning_rate': 1.5150069568674222e-05, 'epoch': 1.21}         \n",
      "{'loss': 2.0812, 'learning_rate': 1.5110316040548601e-05, 'epoch': 1.22}        \n",
      "{'loss': 2.0898, 'learning_rate': 1.507056251242298e-05, 'epoch': 1.23}         \n",
      "{'loss': 2.1209, 'learning_rate': 1.5030808984297357e-05, 'epoch': 1.24}        \n",
      "{'loss': 2.1219, 'learning_rate': 1.4991055456171736e-05, 'epoch': 1.25}        \n",
      "{'loss': 2.07, 'learning_rate': 1.4951301928046116e-05, 'epoch': 1.26}          \n",
      "{'loss': 2.1126, 'learning_rate': 1.4911548399920495e-05, 'epoch': 1.27}        \n",
      "{'loss': 2.1034, 'learning_rate': 1.4871794871794874e-05, 'epoch': 1.28}        \n",
      "{'loss': 2.1361, 'learning_rate': 1.4832041343669253e-05, 'epoch': 1.29}        \n",
      "{'loss': 2.0841, 'learning_rate': 1.4792287815543632e-05, 'epoch': 1.3}         \n",
      "{'loss': 2.1386, 'learning_rate': 1.475253428741801e-05, 'epoch': 1.31}         \n",
      "{'loss': 2.1847, 'learning_rate': 1.4712780759292388e-05, 'epoch': 1.32}        \n",
      "{'loss': 2.1184, 'learning_rate': 1.4673027231166767e-05, 'epoch': 1.33}        \n",
      "{'loss': 2.0964, 'learning_rate': 1.4633273703041146e-05, 'epoch': 1.34}        \n",
      "{'loss': 2.1477, 'learning_rate': 1.4593520174915526e-05, 'epoch': 1.35}        \n",
      "{'loss': 2.0802, 'learning_rate': 1.4553766646789905e-05, 'epoch': 1.36}        \n",
      "{'loss': 2.0165, 'learning_rate': 1.4514013118664284e-05, 'epoch': 1.37}        \n",
      "{'loss': 2.1058, 'learning_rate': 1.4474259590538661e-05, 'epoch': 1.38}        \n",
      "{'loss': 2.1232, 'learning_rate': 1.443450606241304e-05, 'epoch': 1.39}         \n",
      "{'loss': 2.1013, 'learning_rate': 1.439475253428742e-05, 'epoch': 1.4}          \n",
      "{'loss': 2.0986, 'learning_rate': 1.4354999006161798e-05, 'epoch': 1.41}        \n",
      "{'loss': 2.0948, 'learning_rate': 1.4315245478036177e-05, 'epoch': 1.42}        \n",
      "{'loss': 2.136, 'learning_rate': 1.4275491949910556e-05, 'epoch': 1.43}         \n",
      "{'loss': 2.1041, 'learning_rate': 1.4235738421784936e-05, 'epoch': 1.44}        \n",
      "{'loss': 2.0928, 'learning_rate': 1.4195984893659313e-05, 'epoch': 1.45}        \n",
      "{'loss': 2.2001, 'learning_rate': 1.4156231365533692e-05, 'epoch': 1.46}        \n",
      "{'loss': 2.0987, 'learning_rate': 1.4116477837408071e-05, 'epoch': 1.47}        \n",
      "{'loss': 2.058, 'learning_rate': 1.407672430928245e-05, 'epoch': 1.48}          \n",
      "{'loss': 2.1135, 'learning_rate': 1.403697078115683e-05, 'epoch': 1.49}         \n",
      "{'loss': 2.0641, 'learning_rate': 1.3997217253031208e-05, 'epoch': 1.5}         \n",
      "{'loss': 2.1445, 'learning_rate': 1.3957463724905587e-05, 'epoch': 1.51}        \n",
      "{'loss': 2.0839, 'learning_rate': 1.3917710196779965e-05, 'epoch': 1.52}        \n",
      "{'loss': 2.1085, 'learning_rate': 1.3877956668654344e-05, 'epoch': 1.53}        \n",
      "{'loss': 2.1404, 'learning_rate': 1.3838203140528723e-05, 'epoch': 1.54}        \n",
      "{'loss': 2.1669, 'learning_rate': 1.3798449612403102e-05, 'epoch': 1.55}        \n",
      "{'loss': 2.2034, 'learning_rate': 1.3758696084277481e-05, 'epoch': 1.56}        \n",
      "{'loss': 2.1019, 'learning_rate': 1.371894255615186e-05, 'epoch': 1.57}         \n",
      "{'loss': 2.1222, 'learning_rate': 1.367918902802624e-05, 'epoch': 1.58}         \n",
      "{'loss': 2.1263, 'learning_rate': 1.3639435499900617e-05, 'epoch': 1.59}        \n",
      " 32%|██████████▏                     | 80000/251550 [2:56:48<6:56:13,  6.87it/s][INFO|trainer.py:2090] 2023-05-15 14:57:19,485 >> Saving model checkpoint to checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000\n",
      "[INFO|configuration_utils.py:430] 2023-05-15 14:57:19,489 >> Configuration saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000/config.json\n",
      "[INFO|modeling_utils.py:1074] 2023-05-15 14:57:20,584 >> Model weights saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2023-05-15 14:57:20,756 >> tokenizer config file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2023-05-15 14:57:20,770 >> Special tokens file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2125] 2023-05-15 14:57:20,781 >> added tokens file saved in checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100/checkpoint-80000/added_tokens.json\n",
      "{'loss': 2.1745, 'learning_rate': 1.3599681971774996e-05, 'epoch': 1.6}         \n",
      "{'loss': 2.1009, 'learning_rate': 1.3559928443649375e-05, 'epoch': 1.61}        \n",
      "{'loss': 2.1747, 'learning_rate': 1.3520174915523754e-05, 'epoch': 1.62}        \n",
      "{'loss': 2.0606, 'learning_rate': 1.3480421387398133e-05, 'epoch': 1.63}        \n",
      "{'loss': 2.103, 'learning_rate': 1.3440667859272512e-05, 'epoch': 1.64}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1015, 'learning_rate': 1.3400914331146891e-05, 'epoch': 1.65}        \n",
      "{'loss': 2.0688, 'learning_rate': 1.3361160803021268e-05, 'epoch': 1.66}        \n",
      "{'loss': 2.1091, 'learning_rate': 1.3321407274895648e-05, 'epoch': 1.67}        \n",
      "{'loss': 2.0806, 'learning_rate': 1.3281653746770027e-05, 'epoch': 1.68}        \n",
      "{'loss': 2.1158, 'learning_rate': 1.3241900218644406e-05, 'epoch': 1.69}        \n",
      "{'loss': 2.123, 'learning_rate': 1.3202146690518785e-05, 'epoch': 1.7}          \n",
      "{'loss': 2.1195, 'learning_rate': 1.3162393162393164e-05, 'epoch': 1.71}        \n",
      "{'loss': 2.064, 'learning_rate': 1.3122639634267543e-05, 'epoch': 1.72}         \n",
      "{'loss': 2.1337, 'learning_rate': 1.308288610614192e-05, 'epoch': 1.73}         \n",
      "{'loss': 2.1004, 'learning_rate': 1.30431325780163e-05, 'epoch': 1.74}          \n",
      "{'loss': 2.0676, 'learning_rate': 1.3003379049890678e-05, 'epoch': 1.75}        \n",
      "{'loss': 2.0996, 'learning_rate': 1.2963625521765058e-05, 'epoch': 1.76}        \n",
      "{'loss': 2.1454, 'learning_rate': 1.2923871993639437e-05, 'epoch': 1.77}        \n",
      "{'loss': 2.1388, 'learning_rate': 1.2884118465513816e-05, 'epoch': 1.78}        \n",
      "{'loss': 2.113, 'learning_rate': 1.2844364937388195e-05, 'epoch': 1.79}         \n",
      "{'loss': 2.1257, 'learning_rate': 1.2804611409262572e-05, 'epoch': 1.8}         \n",
      "{'loss': 2.1131, 'learning_rate': 1.2764857881136951e-05, 'epoch': 1.81}        \n",
      "{'loss': 2.0862, 'learning_rate': 1.272510435301133e-05, 'epoch': 1.82}         \n",
      "{'loss': 2.0477, 'learning_rate': 1.268535082488571e-05, 'epoch': 1.83}         \n",
      "{'loss': 2.1047, 'learning_rate': 1.2645597296760088e-05, 'epoch': 1.84}        \n",
      "{'loss': 2.1272, 'learning_rate': 1.2605843768634468e-05, 'epoch': 1.85}        \n",
      "{'loss': 2.1044, 'learning_rate': 1.2566090240508847e-05, 'epoch': 1.86}        \n",
      "{'loss': 2.0731, 'learning_rate': 1.2526336712383224e-05, 'epoch': 1.87}        \n",
      "{'loss': 2.0435, 'learning_rate': 1.2486583184257603e-05, 'epoch': 1.88}        \n",
      "{'loss': 2.1792, 'learning_rate': 1.2446829656131982e-05, 'epoch': 1.89}        \n",
      "{'loss': 2.0818, 'learning_rate': 1.2407076128006361e-05, 'epoch': 1.9}         \n",
      "{'loss': 2.1379, 'learning_rate': 1.236732259988074e-05, 'epoch': 1.91}         \n",
      "{'loss': 2.1134, 'learning_rate': 1.232756907175512e-05, 'epoch': 1.92}         \n",
      "{'loss': 2.1164, 'learning_rate': 1.2287815543629498e-05, 'epoch': 1.93}        \n",
      "{'loss': 2.1666, 'learning_rate': 1.2248062015503876e-05, 'epoch': 1.94}        \n",
      "{'loss': 2.0858, 'learning_rate': 1.2208308487378255e-05, 'epoch': 1.95}        \n",
      "{'loss': 2.1078, 'learning_rate': 1.2168554959252634e-05, 'epoch': 1.96}        \n",
      "{'loss': 2.0391, 'learning_rate': 1.2128801431127013e-05, 'epoch': 1.97}        \n",
      "{'loss': 2.0823, 'learning_rate': 1.2089047903001392e-05, 'epoch': 1.98}        \n",
      "{'loss': 2.1395, 'learning_rate': 1.2049294374875771e-05, 'epoch': 1.99}        \n",
      "{'loss': 2.041, 'learning_rate': 1.200954084675015e-05, 'epoch': 2.0}           \n",
      "{'loss': 2.023, 'learning_rate': 1.1969787318624528e-05, 'epoch': 2.01}         \n",
      "{'loss': 1.9375, 'learning_rate': 1.1930033790498907e-05, 'epoch': 2.02}        \n",
      "{'loss': 2.0394, 'learning_rate': 1.1890280262373286e-05, 'epoch': 2.03}        \n",
      "{'loss': 1.976, 'learning_rate': 1.1850526734247665e-05, 'epoch': 2.04}         \n",
      "{'loss': 1.9586, 'learning_rate': 1.1810773206122044e-05, 'epoch': 2.05}        \n",
      "{'loss': 2.0446, 'learning_rate': 1.1771019677996423e-05, 'epoch': 2.06}        \n",
      "{'loss': 2.0238, 'learning_rate': 1.1731266149870802e-05, 'epoch': 2.07}        \n",
      "{'loss': 2.0061, 'learning_rate': 1.169151262174518e-05, 'epoch': 2.08}         \n",
      "{'loss': 2.0053, 'learning_rate': 1.1651759093619559e-05, 'epoch': 2.09}        \n",
      "{'loss': 1.966, 'learning_rate': 1.1612005565493938e-05, 'epoch': 2.1}          \n",
      "{'loss': 1.9741, 'learning_rate': 1.1572252037368317e-05, 'epoch': 2.11}        \n",
      "{'loss': 2.0389, 'learning_rate': 1.1532498509242696e-05, 'epoch': 2.12}        \n",
      "{'loss': 2.0196, 'learning_rate': 1.1492744981117075e-05, 'epoch': 2.13}        \n",
      "{'loss': 1.9455, 'learning_rate': 1.1452991452991454e-05, 'epoch': 2.14}        \n",
      "{'loss': 1.9547, 'learning_rate': 1.1413237924865831e-05, 'epoch': 2.15}        \n",
      "{'loss': 1.9117, 'learning_rate': 1.137348439674021e-05, 'epoch': 2.16}         \n",
      "{'loss': 2.0913, 'learning_rate': 1.133373086861459e-05, 'epoch': 2.17}         \n",
      "{'loss': 1.9711, 'learning_rate': 1.1293977340488969e-05, 'epoch': 2.18}        \n",
      "{'loss': 1.9722, 'learning_rate': 1.1254223812363348e-05, 'epoch': 2.19}        \n",
      "{'loss': 2.0596, 'learning_rate': 1.1214470284237727e-05, 'epoch': 2.2}         \n",
      "{'loss': 1.9526, 'learning_rate': 1.1174716756112106e-05, 'epoch': 2.21}        \n",
      "{'loss': 1.9712, 'learning_rate': 1.1134963227986483e-05, 'epoch': 2.22}        \n",
      "{'loss': 1.9496, 'learning_rate': 1.1095209699860862e-05, 'epoch': 2.23}        \n",
      "{'loss': 1.9887, 'learning_rate': 1.1055456171735241e-05, 'epoch': 2.24}        \n",
      "{'loss': 1.9848, 'learning_rate': 1.101570264360962e-05, 'epoch': 2.25}         \n",
      "{'loss': 1.9867, 'learning_rate': 1.0975949115484e-05, 'epoch': 2.26}           \n",
      "{'loss': 1.9641, 'learning_rate': 1.0936195587358379e-05, 'epoch': 2.27}        \n",
      "{'loss': 2.0245, 'learning_rate': 1.0896442059232758e-05, 'epoch': 2.28}        \n",
      "{'loss': 2.0352, 'learning_rate': 1.0856688531107135e-05, 'epoch': 2.29}        \n",
      "{'loss': 2.0222, 'learning_rate': 1.0816935002981514e-05, 'epoch': 2.3}         \n",
      "{'loss': 2.0185, 'learning_rate': 1.0777181474855893e-05, 'epoch': 2.31}        \n",
      "{'loss': 1.961, 'learning_rate': 1.0737427946730272e-05, 'epoch': 2.32}         \n",
      "{'loss': 1.9379, 'learning_rate': 1.0697674418604651e-05, 'epoch': 2.33}        \n",
      "{'loss': 1.9228, 'learning_rate': 1.065792089047903e-05, 'epoch': 2.34}         \n",
      "{'loss': 1.9862, 'learning_rate': 1.061816736235341e-05, 'epoch': 2.35}         \n",
      "{'loss': 1.989, 'learning_rate': 1.0578413834227787e-05, 'epoch': 2.36}         \n",
      "{'loss': 1.9497, 'learning_rate': 1.0538660306102166e-05, 'epoch': 2.37}        \n",
      "{'loss': 2.0434, 'learning_rate': 1.0498906777976545e-05, 'epoch': 2.38}        \n",
      "{'loss': 1.9588, 'learning_rate': 1.0459153249850924e-05, 'epoch': 2.39}        \n",
      "{'loss': 2.0089, 'learning_rate': 1.0419399721725303e-05, 'epoch': 2.4}         \n",
      "{'loss': 1.9427, 'learning_rate': 1.0379646193599682e-05, 'epoch': 2.41}        \n",
      "{'loss': 2.0121, 'learning_rate': 1.0339892665474061e-05, 'epoch': 2.42}        \n",
      "{'loss': 2.0273, 'learning_rate': 1.0300139137348442e-05, 'epoch': 2.42}        \n",
      "{'loss': 2.0241, 'learning_rate': 1.0260385609222821e-05, 'epoch': 2.43}        \n",
      "{'loss': 2.0287, 'learning_rate': 1.0220632081097199e-05, 'epoch': 2.44}        \n",
      "{'loss': 2.0118, 'learning_rate': 1.0180878552971578e-05, 'epoch': 2.45}        \n",
      "{'loss': 2.0019, 'learning_rate': 1.0141125024845957e-05, 'epoch': 2.46}        \n",
      "{'loss': 2.018, 'learning_rate': 1.0101371496720336e-05, 'epoch': 2.47}         \n",
      "{'loss': 2.0202, 'learning_rate': 1.0061617968594715e-05, 'epoch': 2.48}        \n",
      "{'loss': 1.9723, 'learning_rate': 1.0021864440469094e-05, 'epoch': 2.49}        \n",
      "{'loss': 1.9417, 'learning_rate': 9.982110912343471e-06, 'epoch': 2.5}          \n",
      " 50%|███████████████▌               | 126393/251550 [4:51:04<5:12:55,  6.67it/s]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-base\" \\\n",
    "    --train_file=\"data/train_structure_reddit.csv\" \\\n",
    "    --validation_file=\"data/val_structure_reddit.csv\" \\\n",
    "    --text_column=\"history_aug\" \\\n",
    "    --summary_column=\"response_aug\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=100. \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-base\" \\\n",
    "    --train_file=\"data/train_structure_reddit.csv\" \\\n",
    "    --validation_file=\"data/val_structure_reddit.csv\" \\\n",
    "    --text_column=\"history_aug\" \\\n",
    "    --summary_column=\"response\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=0. \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-base\" \\\n",
    "    --train_file=\"data/train_structure_reddit.csv\" \\\n",
    "    --validation_file=\"data/val_structure_reddit.csv\" \\\n",
    "    --text_column=\"history\" \\\n",
    "    --summary_column=\"response\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=0. \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_or_path = 'checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100'\n",
    "model_name_or_path = 'checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name_or_path).train(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top(text, model, tokenizer, num_beams=4,  max_source_len=1024, max_target_length=64, top_k=50, top_p=1):\n",
    "    inputs = tokenizer([text], max_length=max_source_len, return_tensors=\"pt\", truncation=True, padding = False).to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], do_sample=True,num_beams=num_beams,\n",
    "                                 max_length=max_target_length, top_k=top_k, top_p=top_p)\n",
    "    pred = tokenizer.batch_decode(summary_ids, clean_up_tokenization_spaces=False)[0]\n",
    "    pred = re.sub(r'\\s+', ' ', pred).replace('</s>', '').replace('<s>', '').strip()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/val_structure_reddit.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data['history_aug'].values\n",
    "y_test = test_data['response_aug'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<s1> <u1> <to:u1> <init> People act like DC doesn’t make good animated movies, I’d love it if they at least gave us a conclusion, heck it might be even work better animated. </s> <s2> <u2> <to:u1> <unk> I'd watch the hell out of an animated wrap up of the Snyderverse. </s> <s3> <u3> <to:u1>\",\n",
       " \"<elaboration> TBH Superman/Batman: Apocalypse is a better film than most of the live action stuff DC has put out. Animated directed by Snyder would be an interesting way to go hell I'm still hoping the do Batman 89 animated with Michael Keaton and Michelle Pfieffer reprising their roles.\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 200\n",
    "X_test[k], y_test[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>We know they are done. But it will have been a very confusing 2 seasons.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_top(X_test[k], model, tokenizer, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#model_name_or_path = 'checkpoints/structure_custom_bart_convokit_bs_1_2_lr_2e5_ep_5_noisy_0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/val_structure_reddit.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top(text, model, tokenizer, num_beams=4,  max_source_len=1024, max_target_length=64, top_k=50, top_p=1):\n",
    "    inputs = tokenizer([text], max_length=max_source_len, return_tensors=\"pt\", truncation=True, padding = False).to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], do_sample=True,num_beams=num_beams,\n",
    "                                 max_length=max_target_length, top_k=top_k, top_p=top_p)\n",
    "    pred = tokenizer.batch_decode(summary_ids, clean_up_tokenization_spaces=False)[0]\n",
    "    pred = re.sub(r'\\s+', ' ', pred).replace('</s>', '').replace('<s>', '').strip()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2col = {\n",
    "    \"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp/checkpoint-160000\": \"history_aug\",\n",
    "    \"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100\": \"history_aug\",\n",
    "    \"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut\": \"history_aug\",\n",
    "    \"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels\": \"history\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name_or_path in [\n",
    "    \"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_0_cp\",\n",
    "    #\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100\",\n",
    "    #\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut\",\n",
    "    #\"checkpoint/structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels\",\n",
    "]:\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name_or_path).train(False).to(device)\n",
    "    \n",
    "    X_test = test_data[name2col[model_name_or_path]].values\n",
    "    #y_test = test_data['structure'].values\n",
    "    \n",
    "    preds = []\n",
    "    for i, text in tqdm(enumerate(X_test), total=len(X_test)):\n",
    "        #try:\n",
    "        preds.append([text, generate_top(text, model, tokenizer, top_k=50, num_beams=1)])\n",
    "#         except:\n",
    "#             print(i)\n",
    "#             preds.append([text, 'err'])\n",
    "#             continue\n",
    "            \n",
    "    with open('predictions/{}.pkl'.format(model_name_or_path.replace('checkpoint/', '').replace('/checkpoint-', '-')), 'wb') as f:\n",
    "        pickle.dump([X_test, preds], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from rouge import Rouge\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(preds):\n",
    "    y_true = np.array([p[0] for p in preds])\n",
    "    y_pred = np.array([p[1] for p in preds])\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = [\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100.pkl\",\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut.pkl\",\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels.pkl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2col = {\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100.pkl\": \"history_aug\",\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut.pkl\": \"history_aug\",\n",
    "    \"structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels.pkl\": \"history\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_100.pkl\n",
      "No errors: 10581\n",
      "Accuracy: 0.441\n",
      "\n",
      "ROUGE-1: 8.89\n",
      "ROUGE-2: 0.58\n",
      "ROUGE-L: 7.96\n",
      "\n",
      "BLEU-1: 8.11\n",
      "BLEU-2: 0.17\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norelut.pkl\n",
      "No errors: 10581\n",
      "Accuracy: 0.155\n",
      "\n",
      "ROUGE-1: 7.76\n",
      "ROUGE-2: 0.54\n",
      "ROUGE-L: 7.07\n",
      "\n",
      "BLEU-1: 6.8\n",
      "BLEU-2: 0.2\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "structure_custom_bart_reddit_bs_1_2_lr_2e5_ep_5_w_norels.pkl\n",
      "No errors: 10581\n",
      "Accuracy: 0.155\n",
      "\n",
      "ROUGE-1: 7.45\n",
      "ROUGE-2: 0.51\n",
      "ROUGE-L: 6.77\n",
      "\n",
      "BLEU-1: 6.47\n",
      "BLEU-2: 0.17\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/val_structure_reddit.csv\", sep='\\t')\n",
    "y_test = test_data['response_aug'].values\n",
    "\n",
    "for res_path in results_paths:\n",
    "    print(res_path)\n",
    "    X_test = test_data[name2col[res_path]].values\n",
    "    \n",
    "    with open('predictions/' + res_path, 'rb') as f:\n",
    "        _, preds = pickle.load(f)\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        if preds[i][1].startswith('<unk>') and preds[i][1][4] != ' ':\n",
    "            preds[i][1] = '<unk> ' + preds[i][1][5:]\n",
    "    \n",
    "    if res_path.endswith('norelut.pkl') or res_path.endswith('norels.pkl'):\n",
    "        for i in range(len(preds)):\n",
    "            preds[i][1] = '<unk> ' + preds[i][1]\n",
    "            \n",
    "    print('No errors:', len([p for p in preds if p[1] != 'err']))\n",
    "    \n",
    "    relations = []\n",
    "    cnt_err = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i][1] != 'err':\n",
    "            pred_rel = preds[i][1].split(' ', 1)[0]\n",
    "            relation = y_test[i].split(' ', 1)[0]\n",
    "            relations.append([relation, pred_rel])\n",
    "            \n",
    "    print('Accuracy:', round(calc_accuracy(relations), 3))\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    hyps, refs = [], []\n",
    "    for i in range(len(preds)):\n",
    "        #try:\n",
    "        if len(preds[i][1].split(' ', 1)) > 1:\n",
    "            hyps.append(preds[i][1].split(' ', 1)[1])\n",
    "        else:\n",
    "            hyps.append('')\n",
    "            \n",
    "        if len(y_test[i].split(' ', 1)) > 1:\n",
    "            refs.append(y_test[i].split(' ', 1)[1])\n",
    "        else:\n",
    "            refs.append('')\n",
    "        #except:\n",
    "        #    continue\n",
    "    \n",
    "    gen_ref = zip(hyps, refs)\n",
    "    gen_ref = [_ for _ in gen_ref if not all(j in string.punctuation for j in _[1]) and not all(j in string.punctuation for j in _[0])]\n",
    "    gens, refs  = zip(*gen_ref)\n",
    "    \n",
    "    rouge_res = rouge.get_scores(gens, refs, avg=True, ignore_empty=False)\n",
    "    print()\n",
    "    print('ROUGE-1:', round(100 * rouge_res['rouge-1']['f'], 2))\n",
    "    print('ROUGE-2:', round(100 * rouge_res['rouge-2']['f'], 2))\n",
    "    print('ROUGE-L:', round(100 * rouge_res['rouge-l']['f'], 2))\n",
    "    \n",
    "    mean_bleu = 0\n",
    "    for gen, ref in zip(gens, refs):\n",
    "        mean_bleu += sentence_bleu([word_tokenize(ref)], word_tokenize(gen), weights=[1,0,0,0])\n",
    "    mean_bleu /= len(gens)\n",
    "    print()\n",
    "    print('BLEU-1:', round(100 * mean_bleu, 2))\n",
    "    \n",
    "    mean_bleu = 0\n",
    "    for gen, ref in zip(gens, refs):\n",
    "        mean_bleu += sentence_bleu([word_tokenize(ref)], word_tokenize(gen), weights=[1,1,0,0])\n",
    "    mean_bleu /= len(gens)\n",
    "    print('BLEU-2:', round(100 * mean_bleu, 2))\n",
    "    \n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
