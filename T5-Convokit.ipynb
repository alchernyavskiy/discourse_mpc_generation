{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is constructed in \"BART-Convokit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a142586680e84e348ba3424777124b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7efaaacd4a04eeb941361583fa9b47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff1eab2351f40fd96dc2f6e48f9f19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4576745f88b94a3e815c27296d1da427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name_or_path = \"t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)\n",
    "model =  T5ForConditionalGeneration.from_pretrained(model_name_or_path).to(device) # to check load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/special_tokens_map_convokit.pkl', 'rb') as f:\n",
    "    special_tokens_dict = pickle.load(f)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32193"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added_toks + tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32100, 32101, 32102, 2, 32103, 32104, 32105, 32106, 32107, 32108, 32109, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(['<negativereaction>',\n",
    "     '<other>',\n",
    "     '<appreciation>',\n",
    "     '<unk>',\n",
    "     '<elaboration>',\n",
    "     '<answer>',\n",
    "     '<question>',\n",
    "     '<humor>',\n",
    "     '<announcement>',\n",
    "     '<agreement>',\n",
    "     '<disagreement>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxk3QE9tSjeS"
   },
   "source": [
    "## Train Structure Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "06/19/2022 20:15:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/19/2022 20:15:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_w_100/runs/Jun19_20-15-18_cn-012,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_w_100,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_w_100,\n",
      "save_on_each_node=False,\n",
      "save_steps=80000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/19/2022 20:15:19 - WARNING - datasets.builder - Using custom data configuration default-81207ee423de180e\n",
      "06/19/2022 20:15:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/19/2022 20:15:19 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-81207ee423de180e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "06/19/2022 20:15:19 - WARNING - datasets.builder - Reusing dataset csv (/home/aschernyavskiy/.cache/huggingface/datasets/csv/default-81207ee423de180e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "06/19/2022 20:15:19 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-81207ee423de180e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 251.69it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-06-19 20:15:20,075 >> loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "[INFO|configuration_utils.py:680] 2022-06-19 20:15:20,077 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-06-19 20:15:23,304 >> loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /home/aschernyavskiy/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-06-19 20:15:23,305 >> loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-06-19 20:15:23,305 >> loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-06-19 20:15:23,305 >> loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-06-19 20:15:23,305 >> loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "[INFO|configuration_utils.py:644] 2022-06-19 20:15:23,839 >> loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "[INFO|configuration_utils.py:680] 2022-06-19 20:15:23,840 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2022-06-19 20:15:24,416 >> loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /home/aschernyavskiy/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "CUSTOM T5 with class_weight=100.0\n",
      "[INFO|modeling_utils.py:1694] 2022-06-19 20:15:30,638 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2022-06-19 20:15:30,639 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,644 >> Assigning ['<negativereaction>', '<other>', '<appreciation>', '<unk>', '<elaboration>', '<answer>', '<question>', '<humor>', '<announcement>', '<agreement>', '<disagreement>', '<s1>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>', '<s17>', '<s18>', '<s19>', '<s20>', '<s21>', '<s22>', '<s23>', '<s24>', '<s25>', '<s26>', '<s27>', '<s28>', '<s29>', '<s30>', '<s31>', '<s32>', '<s33>', '<s34>', '<s35>', '<s36>', '<s37>', '<s38>', '<s39>', '<s40>', '<u1>', '<u2>', '<u3>', '<u4>', '<u5>', '<u6>', '<u7>', '<u8>', '<u9>', '<u10>', '<u11>', '<u12>', '<u13>', '<u14>', '<u15>', '<u16>', '<u17>', '<u18>', '<u19>', '<u20>', '<u21>', '<u22>', '<u23>', '<u24>', '<u25>', '<u26>', '<u27>', '<u28>', '<u29>', '<u30>', '<u31>', '<u32>', '<u33>', '<u34>', '<u35>', '<u36>', '<u37>', '<u38>', '<u39>', '<u40>', '<u41>'] to the additional_special_tokens key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <negativereaction> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <other> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <appreciation> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <elaboration> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <answer> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <question> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <humor> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <announcement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <agreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,644 >> Adding <disagreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,645 >> Adding <s21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <s40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,646 >> Adding <u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,647 >> Adding <u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u37> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u38> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u39> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,648 >> Adding <u40> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,649 >> Adding <u41> to the vocabulary\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,649 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,649 >> Adding <s> to the vocabulary\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,650 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,650 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,651 >> Assigning </s> to the sep_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,651 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,652 >> Assigning <s> to the cls_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2022-06-19 20:15:30,652 >> Assigning <mask> to the mask_token key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2022-06-19 20:15:30,653 >> Adding <mask> to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: <negativereaction> <other> <appreciation> <elaboration> <answer> <question> <humor> <announcement> <agreement> <disagreement>\n",
      "06/19/2022 20:15:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-81207ee423de180e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-a5439a9f510cd0f9.arrow\n",
      "06/19/2022 20:15:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-81207ee423de180e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-b81b4fa01299d2b9.arrow\n",
      "/home/aschernyavskiy/anaconda3/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1244] 2022-06-19 20:15:32,074 >> ***** Running training *****\n",
      "[INFO|trainer.py:1245] 2022-06-19 20:15:32,074 >>   Num examples = 81984\n",
      "[INFO|trainer.py:1246] 2022-06-19 20:15:32,074 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1247] 2022-06-19 20:15:32,074 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1248] 2022-06-19 20:15:32,074 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1249] 2022-06-19 20:15:32,074 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1250] 2022-06-19 20:15:32,074 >>   Total optimization steps = 204960\n",
      "{'loss': 8.7606, 'learning_rate': 1.9951209992193602e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.9152, 'learning_rate': 1.99024199843872e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.6102, 'learning_rate': 1.9853629976580797e-05, 'epoch': 0.04}        \n",
      "{'loss': 3.3865, 'learning_rate': 1.9804839968774398e-05, 'epoch': 0.05}        \n",
      "{'loss': 3.2062, 'learning_rate': 1.9756049960967995e-05, 'epoch': 0.06}        \n",
      "{'loss': 3.1773, 'learning_rate': 1.9707259953161596e-05, 'epoch': 0.07}        \n",
      "{'loss': 3.0643, 'learning_rate': 1.9658469945355194e-05, 'epoch': 0.09}        \n",
      "{'loss': 3.0474, 'learning_rate': 1.960967993754879e-05, 'epoch': 0.1}          \n",
      "{'loss': 2.9714, 'learning_rate': 1.9560889929742392e-05, 'epoch': 0.11}        \n",
      "{'loss': 2.9283, 'learning_rate': 1.951209992193599e-05, 'epoch': 0.12}         \n",
      "{'loss': 2.837, 'learning_rate': 1.946330991412959e-05, 'epoch': 0.13}          \n",
      "{'loss': 2.8238, 'learning_rate': 1.9414519906323187e-05, 'epoch': 0.15}        \n",
      "{'loss': 2.8173, 'learning_rate': 1.9365729898516785e-05, 'epoch': 0.16}        \n",
      "{'loss': 2.8322, 'learning_rate': 1.9316939890710386e-05, 'epoch': 0.17}        \n",
      "{'loss': 2.7518, 'learning_rate': 1.9268149882903983e-05, 'epoch': 0.18}        \n",
      "{'loss': 2.8127, 'learning_rate': 1.921935987509758e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.7069, 'learning_rate': 1.917056986729118e-05, 'epoch': 0.21}         \n",
      "{'loss': 2.6803, 'learning_rate': 1.912177985948478e-05, 'epoch': 0.22}         \n",
      "{'loss': 2.5523, 'learning_rate': 1.907298985167838e-05, 'epoch': 0.23}         \n",
      "{'loss': 2.5986, 'learning_rate': 1.9024199843871977e-05, 'epoch': 0.24}        \n",
      "{'loss': 2.5807, 'learning_rate': 1.8975409836065574e-05, 'epoch': 0.26}        \n",
      "{'loss': 2.5877, 'learning_rate': 1.8926619828259175e-05, 'epoch': 0.27}        \n",
      "{'loss': 2.5514, 'learning_rate': 1.8877829820452772e-05, 'epoch': 0.28}        \n",
      "{'loss': 2.5272, 'learning_rate': 1.8829039812646373e-05, 'epoch': 0.29}        \n",
      "{'loss': 2.4343, 'learning_rate': 1.878024980483997e-05, 'epoch': 0.3}          \n",
      "{'loss': 2.5173, 'learning_rate': 1.8731459797033568e-05, 'epoch': 0.32}        \n",
      "{'loss': 2.4743, 'learning_rate': 1.868266978922717e-05, 'epoch': 0.33}         \n",
      "{'loss': 2.4747, 'learning_rate': 1.8633879781420766e-05, 'epoch': 0.34}        \n",
      "{'loss': 2.4673, 'learning_rate': 1.8585089773614363e-05, 'epoch': 0.35}        \n",
      "{'loss': 2.5296, 'learning_rate': 1.8536299765807964e-05, 'epoch': 0.37}        \n",
      "{'loss': 2.4277, 'learning_rate': 1.848750975800156e-05, 'epoch': 0.38}         \n",
      "{'loss': 2.4123, 'learning_rate': 1.8438719750195162e-05, 'epoch': 0.39}        \n",
      "{'loss': 2.4626, 'learning_rate': 1.838992974238876e-05, 'epoch': 0.4}          \n",
      "{'loss': 2.4471, 'learning_rate': 1.8341139734582357e-05, 'epoch': 0.41}        \n",
      "{'loss': 2.3846, 'learning_rate': 1.8292349726775958e-05, 'epoch': 0.43}        \n",
      "{'loss': 2.3904, 'learning_rate': 1.8243559718969555e-05, 'epoch': 0.44}        \n",
      "{'loss': 2.4273, 'learning_rate': 1.8194769711163156e-05, 'epoch': 0.45}        \n",
      "{'loss': 2.3963, 'learning_rate': 1.8145979703356753e-05, 'epoch': 0.46}        \n",
      "{'loss': 2.439, 'learning_rate': 1.809718969555035e-05, 'epoch': 0.48}          \n",
      "{'loss': 2.3949, 'learning_rate': 1.804839968774395e-05, 'epoch': 0.49}         \n",
      "{'loss': 2.3876, 'learning_rate': 1.7999609679937552e-05, 'epoch': 0.5}         \n",
      "{'loss': 2.3148, 'learning_rate': 1.795081967213115e-05, 'epoch': 0.51}         \n",
      "{'loss': 2.355, 'learning_rate': 1.7902029664324747e-05, 'epoch': 0.52}         \n",
      "{'loss': 2.3546, 'learning_rate': 1.7853239656518345e-05, 'epoch': 0.54}        \n",
      " 11%|███▎                           | 22062/204960 [2:47:06<22:12:22,  2.29it/s]"
     ]
    }
   ],
   "source": [
    "# change special tokens map path in run_summarization.py\n",
    "!CUDA_VISIBLE_DEVICES=1 python custom_t5_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"t5-base\" \\\n",
    "    --train_file=\"data/train_structure_convokit.csv\" \\\n",
    "    --validation_file=\"data/val_structure_convokit.csv\" \\\n",
    "    --text_column=\"context\" \\\n",
    "    --summary_column=\"structure\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=100 \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_w_100\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# change special tokens map path in run_summarization.py\n",
    "!CUDA_VISIBLE_DEVICES=0 python custom_t5_scripts_weights/run_summarization.py \\\n",
    "    --model_name_or_path=\"t5-base\" \\\n",
    "    --train_file=\"data/train_structure_convokit_norels.csv\" \\\n",
    "    --validation_file=\"data/val_structure_convokit_norels.csv\" \\\n",
    "    --text_column=\"context\" \\\n",
    "    --summary_column=\"structure\" \\\n",
    "    --max_source_length=1024 \\\n",
    "    --max_target_length=64 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --class_weights=0 \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=5 \\\n",
    "    --output_dir=\"checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_norels\" \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'checkpoints/structure_custom_t5_convokit_bs_1_2_lr_2e5_ep_5_w_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path).train(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top(text, num_beams=4,  max_source_len=1024, max_target_length=64, top_k=50, top_p=1):\n",
    "    inputs = tokenizer([text], max_length=max_source_len, return_tensors=\"pt\", truncation=True, padding = False).to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], do_sample=True,num_beams=num_beams,\n",
    "                                 max_length=max_target_length, top_k=top_k, top_p=top_p)\n",
    "    pred = tokenizer.batch_decode(summary_ids, clean_up_tokenization_spaces=False)[0]\n",
    "    pred = re.sub(r'\\s+', ' ', pred).replace('</s>', '').replace('<s>', '').strip()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/val_structure_convokit.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data['context'].values\n",
    "y_test = test_data['structure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i, text in tqdm(enumerate(X_test), total=len(X_test)):\n",
    "    try:\n",
    "        preds.append([text, generate_top(text, top_k=50, num_beams=1)])\n",
    "    except:\n",
    "        print(i)\n",
    "        preds.append([text, 'err'])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions/{}.pkl'.format(model_name_or_path.replace('checkpoints/', '')), 'wb') as f:\n",
    "    pickle.dump([X_test, preds], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
